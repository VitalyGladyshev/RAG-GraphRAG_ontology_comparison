# –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ—Ü–µ–Ω–∫–∞ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ RAG –Ω–∞ –¥–∞–Ω–Ω—ã—Ö CAD –¥–æ–º–µ–Ω–∞. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å GraphRAG

## –ò–º–ø–æ—Ä—Ç—ã


```python
import os
import sys
import json
import logging
import time
from collections import defaultdict
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple, Union
from datetime import datetime
from dataclasses import dataclass, asdict, field
import pickle
import shutil

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# –í–µ–∫—Ç–æ—Ä–Ω—ã–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
import chromadb

# LangChain
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

# –ú–æ–¥–µ–ª–∏ –∏ ML
import torch
import torch.nn.functional as F
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModel

# Jupyter
from IPython.display import display, HTML
from tqdm.notebook import tqdm

# OpenAI –¥–ª—è custom endpoints
import openai
from openai import AsyncOpenAI

import asyncio
import nest_asyncio
nest_asyncio.apply()
print("‚úÖ nest_asyncio —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–º–µ–Ω–µ–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–º –∫–æ–¥–æ–º –≤ Jupyter")

# RAGAS 0.4+
try:
    from ragas.llms import llm_factory
    from ragas.embeddings.base import embedding_factory
    from ragas.metrics.collections import (
        Faithfulness,              # ‚úÖ –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –æ—Ç–≤–µ—Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
        AnswerRelevancy,           # ‚úÖ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ –∑–∞–ø—Ä–æ—Å—É (—Ç—Ä–µ–±—É–µ—Ç embeddings)
        ContextPrecision,          # ‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (—Ç—Ä–µ–±—É–µ—Ç reference)
        ContextRecall,             # ‚úÖ –ü–æ–ª–Ω–æ—Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (—Ç—Ä–µ–±—É–µ—Ç reference)
        ContextRelevance,          # ‚úÖ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        ResponseGroundedness,      # ‚úÖ –û–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞
        AnswerAccuracy,            # ‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ (—Ç—Ä–µ–±—É–µ—Ç reference)
    )
    RAGAS_AVAILABLE = True
    print("‚úÖ RAGAS 0.4+ —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ (7 –º–µ—Ç—Ä–∏–∫)")
except ImportError as e:
    RAGAS_AVAILABLE = False
    print(f"‚ö†Ô∏è RAGAS –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞: {e}")
    print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install ragas --upgrade")

import warnings
warnings.filterwarnings('ignore')
```

    ‚úÖ nest_asyncio —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–º–µ–Ω–µ–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–º –∫–æ–¥–æ–º –≤ Jupyter
    ‚úÖ RAGAS 0.4+ —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ (7 –º–µ—Ç—Ä–∏–∫)
    


```python
%matplotlib inline
```

## –ö–ª–∞—Å—Å—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π


```python
class JupyterConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Ä–∞–±–æ—Ç—ã –≤ Jupyter Notebook"""
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    DISPLAY_MAX_CHARS = 200
    DISPLAY_TABLE_ROWS = 15
    USE_INTERACTIVE_WIDGETS = True
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    AUTO_SAVE_RESULTS = True
    RESULTS_DIR = "notebook_results"
    
    # –¶–≤–µ—Ç–∞ –¥–ª—è –≤—ã–≤–æ–¥–∞
    SUCCESS_COLOR = "#4CAF50"
    WARNING_COLOR = "#FFA726"
    ERROR_COLOR = "#EF5350"
    INFO_COLOR = "#2196F3"
    
    @classmethod
    def setup_directories(cls):
        """–°–æ–∑–¥–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."""
        Path(cls.RESULTS_DIR).mkdir(parents=True, exist_ok=True)
        print(f"üìÅ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–æ–∑–¥–∞–Ω–∞: {cls.RESULTS_DIR}")
```


```python
class Config:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è RAG-–ø–∞–π–ø–ª–∞–π–Ω–∞."""
    # –ü—É—Ç–∏
    DEFAULT_DATA_DIR = "data/chunks"
    DEFAULT_DB_DIR = "vector_db"
    DEFAULT_MODEL_CACHE = "models/cache"
    DEFAULT_LOGS_DIR = "logs"
    RESULTS_DIR = "notebook_results"
    
    # –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    EMBEDDING_MODEL = "intfloat/multilingual-e5-large"
    EMBEDDING_DIMENSION = 1024
    MAX_SEQUENCE_LENGTH = 512
    
    # –ß–∞–Ω–∫–∏–Ω–≥
    CHUNK_SIZE = 500
    CHUNK_OVERLAP = 50
    
    # –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞
    COLLECTION_NAME = "document_chunks"
    DISTANCE_METRIC = "cosine"
    
    # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
    BATCH_SIZE = 32
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    USE_FP16 = True if torch.cuda.is_available() else False
    
    # –ü–æ–∏—Å–∫
    DEFAULT_TOP_K = 5
    SIMILARITY_THRESHOLD = 0.7
    
    # LLM –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    LLM_API_BASE = "https://api.vsegpt.ru/v1"
    LLM_API_KEY = "sk-or-vv-eea69f0496482e74e9e5a4cccb233dd0128e98c305328e0a56a1e638f5613b99"
    
    # –ú–æ–¥–µ–ª–∏
    GENERATION_MODEL = "google/gemma-3-27b-it"
    EVALUATION_MODEL = "openai/gpt-5-mini"
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    GENERATION_TEMPERATURE = 0.7
    GENERATION_MAX_TOKENS = 1024
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ—Ü–µ–Ω–∫–∏
    EVALUATION_TEMPERATURE = 0.0  # –î–ª—è –æ—Ü–µ–Ω–∫–∏ –Ω—É–∂–Ω–∞ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
    EVALUATION_MAX_TOKENS = 512
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    LOG_LEVEL = "INFO"
    
    @classmethod
    def setup_directories(cls):
        """–°–æ–∑–¥–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
        for directory in [cls.DEFAULT_DATA_DIR, cls.DEFAULT_DB_DIR, 
                         cls.DEFAULT_MODEL_CACHE, cls.DEFAULT_LOGS_DIR,
                         cls.RESULTS_DIR]:
            Path(directory).mkdir(parents=True, exist_ok=True)
```

## –ö–ª–∞—Å—Å—ã –¥–∞–Ω–Ω—ã—Ö


```python
@dataclass
class SearchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ."""
    id: str
    content: str
    similarity_score: float
    metadata: Dict[str, Any]
    embedding: Optional[List[float]] = None

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)
```


```python
@dataclass
class RAGEvaluationResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ü–µ–Ω–∫–∏ –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞."""
    query: str
    answer: str
    context: List[str]
    ground_truth: Optional[str] = None
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    faithfulness: Optional[float] = None
    answer_relevancy: Optional[float] = None
    context_precision: Optional[float] = None
    context_recall: Optional[float] = None
    context_relevance: Optional[float] = None
    response_groundedness: Optional[float] = None
    answer_accuracy: Optional[float] = None
    
    metadata: Dict[str, Any] = field(default_factory=dict)
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    
    def get_average_score(self) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª –ø–æ –≤—Å–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–º –º–µ—Ç—Ä–∏–∫–∞–º."""
        scores = [
            self.faithfulness,
            self.answer_relevancy,
            self.context_precision,
            self.context_recall,
            self.context_relevance,
            self.response_groundedness,
            self.answer_accuracy,
        ]
        valid_scores = [s for s in scores if s is not None]
        return float(np.mean(valid_scores)) if valid_scores else 0.0
    
    def to_dict(self) -> Dict:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Å–ª–æ–≤–∞—Ä—å."""
        return asdict(self)
```

## LLM –∫–ª–∏–µ–Ω—Ç


```python
class UniversalLLMClient:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLM —á–µ—Ä–µ–∑ OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–µ API.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç custom endpoints (–Ω–∞–ø—Ä–∏–º–µ—Ä, vsegpt.ru).
    """
    
    def __init__(self,
                 model: str,
                 api_key: str,
                 api_base: str = "https://api.openai.com/v1",
                 temperature: float = 0.7,
                 max_tokens: int = 1024,
                 timeout: int = 120):
        """
        Args:
            model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "openai/gpt-4o-mini")
            api_key: API –∫–ª—é—á
            api_base: Base URL API
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            timeout: –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        """
        self.model = model
        self.api_key = api_key
        self.api_base = api_base
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.timeout = timeout
        self.logger = logging.getLogger(__name__)
        
        # –°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π OpenAI –∫–ª–∏–µ–Ω—Ç
        self.client = openai.OpenAI(
            api_key=api_key,
            base_url=api_base,
            timeout=timeout,
            max_retries=3
        )
        
        # –°–æ–∑–¥–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π OpenAI –∫–ª–∏–µ–Ω—Ç –¥–ª—è RAGAS
        self.async_client = AsyncOpenAI(
            api_key=api_key,
            base_url=api_base,
            timeout=timeout,
            max_retries=3,
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            default_headers={
                "X-Max-Tokens": str(max_tokens)
            }
        )
        
        self.logger.info(f"‚úÖ LLM –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {model} @ {api_base}")
        self.logger.info(f"‚öôÔ∏è  max_tokens={max_tokens}, temperature={temperature}")
    
    def generate(self,
                prompt: str,
                system_prompt: str = None,
                temperature: float = None,
                max_tokens: int = None) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–º–ø—Ç–∞.
        
        Args:
            prompt: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –¥–µ—Ñ–æ–ª—Ç–Ω—É—é)
            max_tokens: –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤ (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π)
        
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        messages = []
        
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        messages.append({"role": "user", "content": prompt})
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature or self.temperature,
                max_tokens=max_tokens or self.max_tokens
            )
            
            generated_text = response.choices[0].message.content
            
            self.logger.debug(f"‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ ({len(generated_text)} —Å–∏–º–≤–æ–ª–æ–≤)")
            return generated_text.strip()
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            raise
    
    def get_ragas_llm(self):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç RAGAS-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π LLM –æ–±—ä–µ–∫—Ç.
        """
        try:
            # –°–æ–∑–¥–∞–µ–º –æ–±–µ—Ä—Ç–∫—É —Å –Ω—É–∂–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            ragas_llm = llm_factory(
                model=self.model,
                client=self.async_client,
                # –ü–µ—Ä–µ–¥–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                temperature=self.temperature,
                max_tokens=self.max_tokens,
            )
            
            self.logger.info(f"‚úÖ RAGAS LLM —Å–æ–∑–¥–∞–Ω —Å max_tokens={self.max_tokens}")
            return ragas_llm
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ RAGAS LLM: {e}")
            # Fallback –∫ –±–∞–∑–æ–≤–æ–º—É –≤–∞—Ä–∏–∞–Ω—Ç—É
            return llm_factory(
                model=self.model,
                client=self.async_client
            )
```


```python
class LLMGenerator:
    """
    –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è RAG —Å–∏—Å—Ç–µ–º—ã.
    """
    
    def __init__(self,
                 llm_client: UniversalLLMClient,
                 system_prompt: str = None):
        """
        Args:
            llm_client: –ö–ª–∏–µ–Ω—Ç LLM
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        """
        self.llm_client = llm_client
        self.logger = logging.getLogger(__name__)
        
        if system_prompt is None:
            self.system_prompt = """–¢—ã - –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–´–ï –ü–†–ê–í–ò–õ–ê:
1. –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
2. –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º
3. –ù–ï –¥–æ–±–∞–≤–ª—è–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–æ–π –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
4. –û—Ç–≤–µ—á–∞–π —á–µ—Ç–∫–æ, –ø–æ –¥–µ–ª—É –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ
5. –ï—Å–ª–∏ —É–º–µ—Å—Ç–Ω–æ, —Ü–∏—Ç–∏—Ä—É–π —Ñ–∞–∫—Ç—ã –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
6. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ç–æ–º –∂–µ —è–∑—ã–∫–µ, —á—Ç–æ –∏ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        else:
            self.system_prompt = system_prompt
    
    def generate_answer(self,
                       query: str,
                       contexts: List[str],
                       custom_system_prompt: str = None) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–ø—Ä–æ—Å–∞ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤.
        
        Args:
            query: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            contexts: –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤
            custom_system_prompt: –ö–∞—Å—Ç–æ–º–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        """
        if not contexts:
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å."
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã
        combined_context = "\n\n".join([
            f"[–ö–æ–Ω—Ç–µ–∫—Å—Ç {i+1}]:\n{ctx}" 
            for i, ctx in enumerate(contexts[:5])  # –ë–µ—Ä–µ–º —Ç–æ–ø-5
        ])
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
        user_prompt = f"""–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞:
{combined_context}

–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:
{query}

–û—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:"""

        try:
            answer = self.llm_client.generate(
                prompt=user_prompt,
                system_prompt=custom_system_prompt or self.system_prompt
            )
            
            self.logger.info(f"‚úÖ –û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω ({len(answer)} —Å–∏–º–≤–æ–ª–æ–≤)")
            return answer
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}"
    
    def batch_generate(self,
                      queries: List[str],
                      contexts_list: List[List[str]]) -> List[str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã –¥–ª—è –±–∞—Ç—á–∞ –∑–∞–ø—Ä–æ—Å–æ–≤.
        
        Args:
            queries: –°–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤
            contexts_list: –°–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
        """
        answers = []
        for query, contexts in tqdm(zip(queries, contexts_list), 
                                    total=len(queries),
                                    desc="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤"):
            answer = self.generate_answer(query, contexts)
            answers.append(answer)
        
        return answers
```

### –ö–ª–∞—Å—Å –º–µ—Ç—Ä–∏–∫ RAG (RAGAS)


```python
class UniversalRAGEvaluator:
    """
    üéØ –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–´–ô –æ—Ü–µ–Ω—â–∏–∫ RAG —Å–∏—Å—Ç–µ–º —Å RAGAS 0.4+
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç 7 –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫:
    ‚úÖ faithfulness - –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –æ—Ç–≤–µ—Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
    ‚úÖ answer_relevancy - –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ –∑–∞–ø—Ä–æ—Å—É (—Ç—Ä–µ–±—É–µ—Ç embeddings)
    ‚úÖ context_precision - –¢–æ—á–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (—Ç—Ä–µ–±—É–µ—Ç reference)
    ‚úÖ context_recall - –ü–æ–ª–Ω–æ—Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (—Ç—Ä–µ–±—É–µ—Ç reference)
    ‚úÖ context_relevance - –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    ‚úÖ response_groundedness - –û–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞
    ‚úÖ answer_accuracy - –¢–æ—á–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ (—Ç—Ä–µ–±—É–µ—Ç reference)
    """
    
    # ‚úÖ –ü–†–û–í–ï–†–ï–ù–ù–´–ï –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è RAGAS 0.4+
    AVAILABLE_METRICS = [
        "faithfulness",
        "answer_relevancy",
        "context_precision",
        "context_recall",
        "context_relevance",
        "response_groundedness",
        "answer_accuracy",
    ]
    
    def __init__(self, 
                 judge_llm_client: UniversalLLMClient,
                 embedding_model: str = "emb-openai/text-embedding-3-large",
                 metrics: List[str] = None,
                 enable_timing: bool = True):
        """
        Args:
            judge_llm_client: LLM –∫–ª–∏–µ–Ω—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ (Judge)
            embedding_model: –ú–æ–¥–µ–ª—å –¥–ª—è embeddings (–¥–ª—è answer_relevancy)
            metrics: –°–ø–∏—Å–æ–∫ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –±–∞–∑–æ–≤—ã–µ)
        """
        self.judge_llm_client = judge_llm_client
        self.embedding_model = embedding_model
        self.evaluation_results: List[RAGEvaluationResult] = []
        self.logger = logging.getLogger(__name__)
        
        if not RAGAS_AVAILABLE:
            raise ImportError("RAGAS –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install ragas --upgrade")
        
        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (—Ä–∞–±–æ—Ç–∞—é—Ç –±–µ–∑ ground truth)
        self.basic_metrics = [
            "faithfulness",
            "context_relevance",
            "response_groundedness",
        ]
        
        # –ú–µ—Ç—Ä–∏–∫–∏, —Ç—Ä–µ–±—É—é—â–∏–µ embeddings
        self.embedding_metrics = [
            "answer_relevancy",
        ]
        
        # –ú–µ—Ç—Ä–∏–∫–∏, —Ç—Ä–µ–±—É—é—â–∏–µ ground truth
        self.ground_truth_metrics = [
            "context_precision",
            "context_recall",
            "answer_accuracy",
        ]
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        self.metrics_to_use = metrics or self.basic_metrics
        self._validate_metrics()
        
        # –°–æ–∑–¥–∞–µ–º RAGAS LLM –∏ embeddings
        self.ragas_llm = self.judge_llm_client.get_ragas_llm()
        
        # –°–æ–∑–¥–∞–µ–º embeddings —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω—É–∂–Ω—ã
        self.ragas_embeddings = None
        if any(m in self.metrics_to_use for m in self.embedding_metrics):
            try:
                self.ragas_embeddings = embedding_factory(
                    "openai",
                    model=embedding_model,
                    client=self.judge_llm_client.async_client
                )
                self.logger.info(f"‚úÖ Embeddings –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã: {embedding_model}")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ embeddings: {e}")
                # –£–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏, —Ç—Ä–µ–±—É—é—â–∏–µ embeddings
                self.metrics_to_use = [m for m in self.metrics_to_use if m not in self.embedding_metrics]
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏
        self.metric_scorers = {}
        self._initialize_metrics()

        # –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏
        self.enable_timing = enable_timing
        self.metric_timings = defaultdict(list)  # {metric_name: [time1, time2, ...]}
        
        self.logger.info(f"‚úÖ –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π RAG Evaluator –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (RAGAS 0.4+)")
        self.logger.info(f"üìä –ú–µ—Ç—Ä–∏–∫–∏: {', '.join(self.metrics_to_use)}")

        self._nest_asyncio_applied = False
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏–º–µ–Ω—è–µ–º nest_asyncio –¥–ª—è Jupyter
        if self._is_running_in_jupyter():
            self._apply_nest_asyncio()

    def _is_running_in_jupyter(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç Jupyter/IPython –æ–∫—Ä—É–∂–µ–Ω–∏–µ."""
        try:
            from IPython import get_ipython
            ipython = get_ipython()
            return ipython is not None and 'IPKernelApp' in getattr(ipython, 'config', {})
        except (ImportError, AttributeError):
            return False
    
    def _apply_nest_asyncio(self):
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç nest_asyncio –æ–¥–∏–Ω —Ä–∞–∑."""
        if not self._nest_asyncio_applied:
            try:
                import nest_asyncio
                nest_asyncio.apply()
                self._nest_asyncio_applied = True
                self.logger.info("‚úÖ nest_asyncio –ø—Ä–∏–º–µ–Ω–µ–Ω –¥–ª—è Jupyter")
            except ImportError:
                self.logger.warning("‚ö†Ô∏è –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ nest_asyncio: pip install nest_asyncio")
    
    def _validate_metrics(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫."""
        invalid_metrics = set(self.metrics_to_use) - set(self.AVAILABLE_METRICS)
        if invalid_metrics:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏: {invalid_metrics}")
    
    def _initialize_metrics(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—ä–µ–∫—Ç—ã –º–µ—Ç—Ä–∏–∫ —Å LLM/Embeddings."""
        metric_classes = {
            "faithfulness": Faithfulness,
            "answer_relevancy": AnswerRelevancy,
            "context_precision": ContextPrecision,
            "context_recall": ContextRecall,
            "context_relevance": ContextRelevance,
            "response_groundedness": ResponseGroundedness,
            "answer_accuracy": AnswerAccuracy,
        }
        
        for metric_name in self.metrics_to_use:
            if metric_name not in metric_classes:
                continue
            
            try:
                metric_class = metric_classes[metric_name]
                
                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫—É —Å –Ω—É–∂–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                if metric_name in self.embedding_metrics:
                    if self.ragas_embeddings is None:
                        self.logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–∞ –º–µ—Ç—Ä–∏–∫–∞ {metric_name} (–Ω–µ—Ç embeddings)")
                        continue
                    scorer = metric_class(llm=self.ragas_llm, embeddings=self.ragas_embeddings)
                else:
                    scorer = metric_class(llm=self.ragas_llm)
                
                self.metric_scorers[metric_name] = scorer
                self.logger.info(f"‚úÖ –ú–µ—Ç—Ä–∏–∫–∞ {metric_name} –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
                
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–µ—Ç—Ä–∏–∫–∏ {metric_name}: {e}")
    
    async def _evaluate_single_async(self, 
                                     query: str, 
                                     answer: str, 
                                     contexts: List[str],
                                     ground_truth: str = None) -> Dict[str, float]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞.
        
        Args:
            query: –í–æ–ø—Ä–æ—Å
            answer: –û—Ç–≤–µ—Ç
            contexts: –ö–æ–Ω—Ç–µ–∫—Å—Ç—ã
            ground_truth: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        scores = {}
        
        for metric_name, scorer in self.metric_scorers.items():
            start_time = time.time()
            
            try:
                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–µ—Ç—Ä–∏–∫–∏
                if metric_name == "faithfulness":
                    result = await scorer.ascore(
                        user_input=query,
                        response=answer,
                        retrieved_contexts=contexts
                    )
                
                elif metric_name == "answer_relevancy":
                    result = await scorer.ascore(
                        user_input=query,
                        response=answer
                    )
                
                elif metric_name == "context_precision":
                    if not ground_truth:
                        continue
                    result = await scorer.ascore(
                        user_input=query,
                        reference=ground_truth,
                        retrieved_contexts=contexts
                    )
                
                elif metric_name == "context_recall":
                    if not ground_truth:
                        continue
                    result = await scorer.ascore(
                        user_input=query,
                        retrieved_contexts=contexts,
                        reference=ground_truth
                    )
                
                elif metric_name == "context_relevance":
                    result = await scorer.ascore(
                        user_input=query,
                        retrieved_contexts=contexts
                    )
                
                elif metric_name == "response_groundedness":
                    result = await scorer.ascore(
                        response=answer,
                        retrieved_contexts=contexts
                    )
                
                elif metric_name == "answer_accuracy":
                    if not ground_truth:
                        continue
                    result = await scorer.ascore(
                        user_input=query,
                        response=answer,
                        reference=ground_truth
                    )
                
                else:
                    continue
                
                scores[metric_name] = float(result.value)
                
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ {metric_name}: {e}")
                scores[metric_name] = None

            finally:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
                if self.enable_timing:
                    elapsed_time = time.time() - start_time
                    self.metric_timings[metric_name].append(elapsed_time)
        
        return scores
    
    def _evaluate_single(self, query: str, answer: str, contexts: List[str],
                        ground_truth: str = None) -> Dict[str, float]:
        """
        –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏.
        """
        try:
            # –ï—Å—Ç—å –∑–∞–ø—É—â–µ–Ω–Ω—ã–π loop - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
            loop = asyncio.get_running_loop()
            if not self._nest_asyncio_applied:
                self._apply_nest_asyncio()
            return loop.run_until_complete(
                self._evaluate_single_async(query, answer, contexts, ground_truth)
            )
        except RuntimeError:
            # –ù–µ—Ç loop - —Å–æ–∑–¥–∞–µ–º —á–µ—Ä–µ–∑ asyncio.run()
            return asyncio.run(
                self._evaluate_single_async(query, answer, contexts, ground_truth)
            )

    def evaluate(self, 
                queries: List[str], 
                answers: List[str], 
                contexts: List[List[str]],
                ground_truths: List[str] = None,
                show_progress: bool = True) -> Dict[str, float]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –æ—Ü–µ–Ω–∫—É RAG —Å–∏—Å—Ç–µ–º—ã.
        
        Args:
            queries: –°–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤
            answers: –°–ø–∏—Å–æ–∫ –æ—Ç–≤–µ—Ç–æ–≤
            contexts: –°–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤
            ground_truths: –°–ø–∏—Å–æ–∫ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            show_progress: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        if len(queries) != len(answers) or len(queries) != len(contexts):
            raise ValueError("–î–ª–∏–Ω—ã queries, answers –∏ contexts –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å")
        
        if ground_truths is None:
            ground_truths = [None] * len(queries)
        elif len(ground_truths) != len(queries):
            raise ValueError("–î–ª–∏–Ω–∞ ground_truths –¥–æ–ª–∂–Ω–∞ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å queries")
        
        has_ground_truth = any(gt and gt.strip() for gt in ground_truths if gt)
        
        self.logger.info("üöÄ –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ RAGAS 0.4+...")
        self.logger.info(f"‚öôÔ∏è  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å: {self.judge_llm_client.model}")
        self.logger.info(f"üìä –ê–∫—Ç–∏–≤–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫: {len(self.metric_scorers)}")
        self.logger.info(f"üéØ Ground truth: {'–î–∞' if has_ground_truth else '–ù–µ—Ç'}")
        self.logger.info(f"‚è≥ –û—Ü–µ–Ω–∫–∞ {len(queries)} –ø—Ä–∏–º–µ—Ä–æ–≤...")
        self.logger.info("‚ö†Ô∏è  –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç (LLM –≤—ã–∑–æ–≤—ã)")
        
        # –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.evaluation_results.clear()
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π –ø—Ä–∏–º–µ—Ä
        iterator = tqdm(zip(queries, answers, contexts, ground_truths), 
                       total=len(queries),
                       desc="–û—Ü–µ–Ω–∫–∞ –ø—Ä–∏–º–µ—Ä–æ–≤") if show_progress else zip(queries, answers, contexts, ground_truths)
        
        all_scores = {metric: [] for metric in self.metric_scorers.keys()}
        
        for query, answer, context, ground_truth in iterator:
            # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = RAGEvaluationResult(
                query=query,
                answer=answer,
                context=context,
                ground_truth=ground_truth
            )
            
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º
            try:
                scores = self._evaluate_single(query, answer, context, ground_truth)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
                for metric_name, score in scores.items():
                    setattr(result, metric_name, score)
                    if score is not None:
                        all_scores[metric_name].append(score)
                
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –ø—Ä–∏–º–µ—Ä–∞: {e}")
            
            self.evaluation_results.append(result)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        aggregated = self._compute_aggregated_metrics(all_scores)
        
        self.logger.info("‚úÖ –û—Ü–µ–Ω–∫–∞ RAGAS –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        return aggregated
    
    def _compute_aggregated_metrics(self, all_scores: Dict[str, List[float]]) -> Dict[str, float]:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏."""
        aggregated = {}
        
        for metric_name, scores in all_scores.items():
            if scores:
                aggregated[f"{metric_name}_mean"] = float(np.mean(scores))
                aggregated[f"{metric_name}_std"] = float(np.std(scores))
                aggregated[f"{metric_name}_min"] = float(np.min(scores))
                aggregated[f"{metric_name}_max"] = float(np.max(scores))
                aggregated[f"{metric_name}_count"] = len(scores)
        
        mean_values = [v for k, v in aggregated.items() if k.endswith('_mean')]
        if mean_values:
            aggregated['overall_mean'] = float(np.mean(mean_values))
        
        return aggregated
    
    def get_detailed_results(self) -> pd.DataFrame:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –≤–∏–¥–µ DataFrame."""
        data = []
        for result in self.evaluation_results:
            row = {
                'query': result.query[:100] + '...' if len(result.query) > 100 else result.query,
                'answer': result.answer[:100] + '...' if len(result.answer) > 100 else result.answer,
            }
            
            for metric_name in self.AVAILABLE_METRICS:
                value = getattr(result, metric_name, None)
                row[metric_name] = value
            
            row['average_score'] = result.get_average_score()
            data.append(row)
        
        return pd.DataFrame(data)
    
    def save_results(self, filepath: str = None) -> Path:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏."""
        if filepath is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = Path("results") / f"ragas_evaluation_{timestamp}.json"
        else:
            filepath = Path(filepath)
        
        filepath.parent.mkdir(parents=True, exist_ok=True)
        results_data = [result.to_dict() for result in self.evaluation_results]
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {filepath}")
        return filepath
    
    def display_results(self, max_examples: int = 5):
        """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ."""
        if not self.evaluation_results:
            print("‚ö†Ô∏è –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è")
            return
        
        df = self.get_detailed_results()
        
        # –¶–≤–µ—Ç–∞ –¥–ª—è –º–µ—Ç—Ä–∏–∫
        metric_colors = {
            "faithfulness": "#4CAF50",
            "answer_relevancy": "#2196F3",
            "context_precision": "#FFA726",
            "context_recall": "#9C27B0",
            "context_relevance": "#FF5722",
            "response_groundedness": "#795548",
            "answer_accuracy": "#F44336",
        }
        
        html = f"""
        <div style="border: 2px solid #2196F3; border-radius: 12px; padding: 20px; margin: 15px 0; background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%);">
            <h2 style="color: #1565C0; margin-top: 0;">üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ RAGAS 0.4+</h2>
            <p style="color: #666;"><strong>–í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤:</strong> {len(self.evaluation_results)} | 
               <strong>–ú–µ—Ç—Ä–∏–∫:</strong> {len([m for m in self.AVAILABLE_METRICS if m in df.columns and df[m].notna().any()])}</p>
        """
        
        html += '<div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 15px; margin: 20px 0;">'
        
        for metric_name in self.AVAILABLE_METRICS:
            if metric_name in df.columns:
                values = df[metric_name].dropna()
                if len(values) > 0:
                    mean_val = values.mean()
                    color = metric_colors.get(metric_name, "#607D8B")
                    
                    html += f"""
                    <div style="text-align: center; padding: 15px; background: white; border-radius: 8px; border: 2px solid {color};">
                        <div style="font-weight: bold; color: {color}; margin-bottom: 8px; text-transform: capitalize;">
                            {metric_name.replace('_', ' ')}
                        </div>
                        <div style="font-size: 32px; font-weight: bold; color: {color};">{mean_val:.3f}</div>
                        <div style="background-color: {color}22; height: 10px; border-radius: 5px; margin-top: 8px;">
                            <div style="background-color: {color}; height: 100%; width: {min(mean_val*100, 100)}%; border-radius: 5px;"></div>
                        </div>
                        <div style="font-size: 12px; color: #666; margin-top: 5px;">
                            Min: {values.min():.3f} | Max: {values.max():.3f}
                        </div>
                    </div>
                    """
        
        html += '</div>'
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
        html += f"""
        <h3 style="color: #1565C0; margin-top: 25px;">üìã –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (—Ç–æ–ø-{max_examples}):</h3>
        """
        
        for i, result in enumerate(self.evaluation_results[:max_examples]):
            avg_score = result.get_average_score()
            border_color = "#4CAF50" if avg_score > 0.7 else "#FFA726" if avg_score > 0.5 else "#F44336"
            
            html += f"""
            <div style="background: white; padding: 15px; margin: 12px 0; border-radius: 8px; border-left: 5px solid {border_color};">
                <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 10px;">
                    <strong style="color: #1a237e; font-size: 1.1em;">–ü—Ä–∏–º–µ—Ä #{i+1}</strong>
                    <span style="background: {border_color}; color: white; padding: 4px 12px; border-radius: 15px; font-weight: bold;">
                        –°—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª: {avg_score:.3f}
                    </span>
                </div>
                
                <div style="background: #f8f9fa; padding: 10px; border-radius: 5px; margin: 8px 0;">
                    <strong style="color: #666;">‚ùì –í–æ–ø—Ä–æ—Å:</strong><br/>
                    <span style="color: #1a237e;">{result.query[:200]}{'...' if len(result.query) > 200 else ''}</span>
                </div>
                
                <div style="background: #e8f5e9; padding: 10px; border-radius: 5px; margin: 8px 0;">
                    <strong style="color: #666;">‚úÖ –û—Ç–≤–µ—Ç:</strong><br/>
                    <span style="color: #2e7d32;">{result.answer[:300]}{'...' if len(result.answer) > 300 else ''}</span>
                </div>
                
                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(150px, 1fr)); gap: 8px; margin-top: 10px;">
            """
            
            for metric_name in self.AVAILABLE_METRICS:
                value = getattr(result, metric_name, None)
                if value is not None:
                    color = metric_colors.get(metric_name, "#607D8B")
                    html += f"""
                    <div style="background: {color}15; padding: 6px 10px; border-radius: 5px; text-align: center; border: 1px solid {color}50;">
                        <div style="font-size: 11px; color: #666; text-transform: uppercase;">{metric_name.replace('_', ' ')}</div>
                        <div style="font-size: 18px; font-weight: bold; color: {color};">{value:.3f}</div>
                    </div>
                    """
            
            html += """
                </div>
            </div>
            """
        
        html += "</div>"
        
        display(HTML(html))
        
        print("\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Å–≤–æ–¥–∫–∞:")
        display(df.describe().style.background_gradient(cmap='viridis'))

    def get_timing_statistics(self) -> pd.DataFrame:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫.
        """
        if not self.metric_timings:
            return pd.DataFrame()
        
        stats = []
        for metric_name, times in self.metric_timings.items():
            if times:
                stats.append({
                    'metric': metric_name,
                    'count': len(times),
                    'total_time_sec': sum(times),
                    'avg_time_sec': np.mean(times),
                    'min_time_sec': np.min(times),
                    'max_time_sec': np.max(times),
                    'std_time_sec': np.std(times),
                })
        
        df = pd.DataFrame(stats)
        if not df.empty:
            df = df.sort_values('avg_time_sec', ascending=False)
        
        return df
    
    def display_timing_analysis(self):
        """
        –û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –∞–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫.
        """
        df = self.get_timing_statistics()
        
        if df.empty:
            print("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è")
            return
        
        total_time = df['total_time_sec'].sum()
        
        html = f"""
        <div style="border: 2px solid #FF9800; border-radius: 12px; padding: 20px; margin: 15px 0; 
                    background: linear-gradient(135deg, #FFF3E0 0%, #FFE0B2 100%);">
            <h2 style="color: #E65100; margin-top: 0;">‚è±Ô∏è –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫</h2>
            <p style="color: #666;"><strong>–û–±—â–µ–µ –≤—Ä–µ–º—è:</strong> {total_time:.1f} —Å–µ–∫ ({total_time/60:.1f} –º–∏–Ω)</p>
            
            <table style="width: 100%; border-collapse: collapse; background: white; margin-top: 15px;">
                <thead>
                    <tr style="background: #FF9800; color: white;">
                        <th style="padding: 10px; text-align: left;">–ú–µ—Ç—Ä–∏–∫–∞</th>
                        <th style="padding: 10px; text-align: center;">–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ</th>
                        <th style="padding: 10px; text-align: right;">–°—Ä–µ–¥–Ω–µ–µ (—Å–µ–∫)</th>
                        <th style="padding: 10px; text-align: right;">–û–±—â–µ–µ (—Å–µ–∫)</th>
                        <th style="padding: 10px; text-align: right;">% –≤—Ä–µ–º–µ–Ω–∏</th>
                    </tr>
                </thead>
                <tbody>
        """
        
        for idx, row in df.iterrows():
            percentage = (row['total_time_sec'] / total_time) * 100
            avg_time = row['avg_time_sec']
            
            # –¶–≤–µ—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤—Ä–µ–º–µ–Ω–∏
            if avg_time > 10:
                bg_color = "#FFCDD2"  # –ö—Ä–∞—Å–Ω—ã–π (–º–µ–¥–ª–µ–Ω–Ω–æ)
            elif avg_time > 5:
                bg_color = "#FFE0B2"  # –û—Ä–∞–Ω–∂–µ–≤—ã–π (—Å—Ä–µ–¥–Ω–µ)
            else:
                bg_color = "#C8E6C9"  # –ó–µ–ª–µ–Ω—ã–π (–±—ã—Å—Ç—Ä–æ)
            
            html += f"""
                <tr style="background: {bg_color};">
                    <td style="padding: 8px; font-weight: bold;">{row['metric']}</td>
                    <td style="padding: 8px; text-align: center;">{row['count']}</td>
                    <td style="padding: 8px; text-align: right;">{row['avg_time_sec']:.2f}</td>
                    <td style="padding: 8px; text-align: right;">{row['total_time_sec']:.1f}</td>
                    <td style="padding: 8px; text-align: right;">{percentage:.1f}%</td>
                </tr>
            """
        
        html += """
                </tbody>
            </table>
            
            <div style="margin-top: 15px; padding: 10px; background: #FFF9C4; border-radius: 5px;">
                <strong>üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:</strong>
                <ul style="margin: 5px 0;">
        """
        
        # –ù–∞—Ö–æ–¥–∏–º —Å–∞–º—ã–µ –º–µ–¥–ª–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        slow_metrics = df[df['avg_time_sec'] > df['avg_time_sec'].median()]['metric'].tolist()
        
        if slow_metrics:
            html += f"<li>üêå <strong>–ú–µ–¥–ª–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:</strong> {', '.join(slow_metrics)}</li>"
        
        # –ú–µ—Ç—Ä–∏–∫–∏ —Å –Ω–∏–∑–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç—å—é
        if 'faithfulness' in df['metric'].values and 'response_groundedness' in df['metric'].values:
            html += """
                <li>‚ö†Ô∏è <strong>faithfulness</strong> –∏ <strong>response_groundedness</strong> 
                –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (1.0) - —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π –∏–∑ –Ω–∏—Ö</li>
            """
        
        html += """
                </ul>
            </div>
        </div>
        """
        
        display(HTML(html))
        
        # –¢–∞–∫–∂–µ –≤—ã–≤–æ–¥–∏–º DataFrame
        print("\nüìä –î–µ—Ç–∞–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞:")
        display(df.style.background_gradient(cmap='YlOrRd', subset=['avg_time_sec']))
    
    def get_recommended_metrics(self, 
                               max_avg_time: float = 10.0,
                               exclude_redundant: bool = True) -> List[str]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
        
        Args:
            max_avg_time: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è (—Å–µ–∫)
            exclude_redundant: –ò—Å–∫–ª—é—á–∏—Ç—å –∏–∑–±—ã—Ç–æ—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã—Ö –º–µ—Ç—Ä–∏–∫
        """
        df = self.get_timing_statistics()
        
        if df.empty:
            return self.metrics_to_use
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        fast_metrics = df[df['avg_time_sec'] <= max_avg_time]['metric'].tolist()
        
        # –ò—Å–∫–ª—é—á–∞–µ–º –∏–∑–±—ã—Ç–æ—á–Ω—ã–µ
        if exclude_redundant:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º faithfulness –∏ response_groundedness
            results_df = self.get_detailed_results()
            
            if 'faithfulness' in results_df.columns and 'response_groundedness' in results_df.columns:
                faith_vals = results_df['faithfulness'].dropna()
                ground_vals = results_df['response_groundedness'].dropna()
                
                # –ï—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ—á—Ç–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ, –∏—Å–∫–ª—é—á–∞–µ–º –æ–¥–Ω—É
                if len(faith_vals) > 0 and len(ground_vals) > 0:
                    correlation = faith_vals.corr(ground_vals)
                    
                    if correlation > 0.95:  # –û—á–µ–Ω—å —Å–∏–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
                        # –û—Å—Ç–∞–≤–ª—è–µ–º –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä—É—é –º–µ—Ç—Ä–∏–∫—É
                        faith_time = df[df['metric'] == 'faithfulness']['avg_time_sec'].values
                        ground_time = df[df['metric'] == 'response_groundedness']['avg_time_sec'].values
                        
                        if len(faith_time) > 0 and len(ground_time) > 0:
                            if faith_time[0] > ground_time[0]:
                                fast_metrics = [m for m in fast_metrics if m != 'faithfulness']
                                self.logger.info("‚ÑπÔ∏è –ò—Å–∫–ª—é—á–µ–Ω–∞ faithfulness (–∏–∑–±—ã—Ç–æ—á–Ω–∞)")
                            else:
                                fast_metrics = [m for m in fast_metrics if m != 'response_groundedness']
                                self.logger.info("‚ÑπÔ∏è –ò—Å–∫–ª—é—á–µ–Ω–∞ response_groundedness (–∏–∑–±—ã—Ç–æ—á–Ω–∞)")
        
        self.logger.info(f"‚úÖ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–µ—Ç—Ä–∏–∫–∏: {', '.join(fast_metrics)}")
        return fast_metrics
```


```python
class TextProcessor:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —á–∞–Ω–∫–æ–≤."""
    
    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ —Ç–µ–∫—Å—Ç–∞.
        
        Args:
            chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
            chunk_overlap: –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        self.logger = logging.getLogger(__name__)
    
    def split_text(self, text: str, metadata: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """
        –†–∞–∑–¥–µ–ª—è–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏.
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            meta –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å —á–∞–Ω–∫–∞–º–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        if metadata is None:
            metadata = {}
        
        # –†–∞–∑–¥–µ–ª—è–µ–º —Ç–µ–∫—Å—Ç
        chunks = self.text_splitter.split_text(text)
        
        # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
        result = []
        for i, chunk in enumerate(chunks):
            chunk_meta = metadata.copy()
            chunk_meta.update({
                "chunk_index": i,
                "total_chunks": len(chunks),
                "chunk_size": len(chunk),
                "is_truncated": len(chunk) >= self.chunk_size
            })
            result.append({
                "text": chunk,
                "metadata": chunk_meta
            })
        
        self.logger.info(f"–†–∞–∑–¥–µ–ª–µ–Ω–æ –Ω–∞ {len(chunks)} —á–∞–Ω–∫–æ–≤")
        return result
    
    def process_file(self, file_path: Path) -> List[Dict[str, Any]]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —Ñ–∞–π–ª.
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
        
        Returns:
            –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞
        """
        try:
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
            encodings = ['utf-8', 'cp1251', 'latin-1']
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        content = f.read().strip()
                    break
                except UnicodeDecodeError:
                    continue
            else:
                self.logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª {file_path}")
                return []
            
            if not content:
                self.logger.warning(f"–§–∞–π–ª {file_path} –ø—É—Å—Ç–æ–π")
                return []
            
            # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            base_metadata = {
                "source_file": file_path.name,
                "file_path": str(file_path),
                "file_size": len(content),
                "encoding": encoding
            }
            
            # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —á–∞–Ω–∫–∏
            chunks = self.split_text(content, base_metadata)
            return chunks
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")
            return []
```


```python
class DataLoader:
    """–ó–∞–≥—Ä—É–∑—á–∏–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤."""
    
    def __init__(self, data_dir: str = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            data_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏
        """
        self.data_dir = Path(data_dir or Config.DEFAULT_DATA_DIR)
        self.logger = logging.getLogger(__name__)
        
        if not self.data_dir.exists():
            self.logger.warning(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {self.data_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            self.data_dir.mkdir(parents=True, exist_ok=True)
    
    def load_files_to_dataframe(self,
                               file_pattern: str = "*.txt",
                               max_files: int = None,
                               process_chunks: bool = True) -> pd.DataFrame:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ñ–∞–π–ª—ã –≤ DataFrame.
        
        Args:
            file_pattern: –®–∞–±–ª–æ–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤
            max_files: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            process_chunks: –†–∞–∑–¥–µ–ª—è—Ç—å –ª–∏ —Ñ–∞–π–ª—ã –Ω–∞ —á–∞–Ω–∫–∏
        
        Returns:
            DataFrame —Å —Ç–µ–∫—Å—Ç–∞–º–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        # –ù–∞—Ö–æ–¥–∏–º —Ñ–∞–π–ª—ã
        files = list(self.data_dir.glob(file_pattern))
        if max_files:
            files = files[:max_files]
        
        if not files:
            self.logger.warning(f"–§–∞–π–ª—ã –ø–æ —à–∞–±–ª–æ–Ω—É {file_pattern} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return pd.DataFrame()
        
        self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(files)} —Ñ–∞–π–ª–æ–≤")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã
        processor = TextProcessor()
        all_chunks = []
        
        for file_path in tqdm(files, desc="–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤"):
            if process_chunks:
                chunks = processor.process_file(file_path)
                all_chunks.extend(chunks)
            else:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª—ã –±–µ–∑ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞ —á–∞–Ω–∫–∏
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read().strip()
                    all_chunks.append({
                        "text": content,
                        "metadata": {
                            "source_file": file_path.name,
                            "file_path": str(file_path)
                        }
                    })
                except Exception as e:
                    self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_path}: {e}")
        
        if not all_chunks:
            self.logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞")
            return pd.DataFrame()
        
        # –°–æ–∑–¥–∞–µ–º DataFrame
        data = []
        for i, chunk in enumerate(all_chunks):
            record = {
                "id": self._generate_chunk_id(chunk["metadata"], i),
                "text": chunk["text"],
                **chunk["metadata"]
            }
            data.append(record)
        
        df = pd.DataFrame(data)
        self.logger.info(f"–°–æ–∑–¥–∞–Ω DataFrame —Å {len(df)} –∑–∞–ø–∏—Å—è–º–∏")
        
        return df
    
    def _generate_chunk_id(self, metadata: Dict, index: int) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è —á–∞–Ω–∫–∞.
        
        Args:
            meta –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∞
            index: –ò–Ω–¥–µ–∫—Å —á–∞–Ω–∫–∞
        
        Returns:
            –£–Ω–∏–∫–∞–ª—å–Ω—ã–π —Å—Ç—Ä–æ–∫–æ–≤—ã–π ID
        """
        # –°–æ–∑–¥–∞–µ–º —Ö—ç—à –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        source = metadata.get("source_file", "unknown")
        chunk_idx = metadata.get("chunk_index", index)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º MD5 –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ—Ä–æ—Ç–∫–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ ID
        hash_input = f"{source}_{chunk_idx}_{datetime.now().timestamp()}"
        hash_digest = hashlib.md5(hash_input.encode()).hexdigest()[:12]
        return f"chunk_{hash_digest}"
    
    def save_dataframe(self, df: pd.DataFrame, output_path: str):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç DataFrame –≤ —Ñ–∞–π–ª.
        
        Args:
            df: DataFrame –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        try:
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ Parquet (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö)
            if output_path.suffix == '.parquet':
                df.to_parquet(output_path, index=False)
            # –ò–ª–∏ –≤ CSV
            elif output_path.suffix == '.csv':
                df.to_csv(output_path, index=False, encoding='utf-8')
            # –ò–ª–∏ –≤ Pickle
            elif output_path.suffix == '.pkl':
                df.to_pickle(output_path)
            else:
                # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Parquet
                output_path = output_path.with_suffix('.parquet')
                df.to_parquet(output_path, index=False)
            
            self.logger.info(f"DataFrame —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {output_path}")
            return output_path
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è DataFrame: {e}")
            raise
```


```python
class MultilingualE5Embedder:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–æ–¥–µ–ª—å—é multilingual-e5-large."""
    
    def __init__(self,
                model_name: str = Config.EMBEDDING_MODEL,
                device: str = None,
                cache_dir: str = None,
                use_fp16: bool = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.
        
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ Hugging Face
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π (cuda/cpu)
            cache_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
            use_fp16: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ FP16 –¥–ª—è GPU
        """
        self.model_name = model_name
        self.device = device or Config.DEVICE
        self.cache_dir = cache_dir or Config.DEFAULT_MODEL_CACHE
        self.use_fp16 = use_fp16 if use_fp16 is not None else Config.USE_FP16
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∫—ç—à–∞
        Path(self.cache_dir).mkdir(parents=True, exist_ok=True)
        
        self.logger = logging.getLogger(__name__)
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä."""
        try:
            self.logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {self.model_name} –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ {self.device}...")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º sentence-transformers –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
            model_kwargs = {}
            if self.device == "cuda" and self.use_fp16:
                model_kwargs["torch_dtype"] = torch.float16
            
            self.model = SentenceTransformer(
                self.model_name,
                device=self.device,
                cache_folder=str(self.cache_dir),
                **model_kwargs
            )
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                cache_dir=str(self.cache_dir)
            )
            
            self.logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {self.device}")
            self.logger.info(f"üìè –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {self.get_embedding_dimension()}")
            
            # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞–º—è—Ç–∏ GPU
            if self.device == "cuda":
                gpu_memory = torch.cuda.memory_allocated() / 1e9
                self.logger.info(f"üéÆ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –ø–∞–º—è—Ç–∏ GPU: {gpu_memory:.2f} GB")
                
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            raise
    
    def get_embedding_dimension(self) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –º–æ–¥–µ–ª–∏."""
        if self.model is None:
            return Config.EMBEDDING_DIMENSION
        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∏–∑ –º–æ–¥–µ–ª–∏
        return self.model.get_sentence_embedding_dimension()
    
    def format_text(self, text: str, text_type: str = "passage") -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç —Å–æ–≥–ª–∞—Å–Ω–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º E5.
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            text_type: –¢–∏–ø —Ç–µ–∫—Å—Ç–∞ (query/passage)
        
        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º
        """
        if text_type not in ["query", "passage"]:
            raise ValueError("text_type –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 'query' –∏–ª–∏ 'passage'")
        
        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –∏ –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å
        cleaned_text = text.strip()
        return f"{text_type}: {cleaned_text}"
    
    def embed_texts(self,
                   texts: List[str],
                   text_type: str = "passage",
                   batch_size: int = 32,
                   show_progress: bool = True) -> np.ndarray:
        """
        –°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤.
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            text_type: –¢–∏–ø —Ç–µ–∫—Å—Ç–æ–≤ (query/passage)
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            show_progress: –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –ª–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
        
        Returns:
            –ú–∞—Å—Å–∏–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        """
        if not texts:
            return np.array([])
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç—ã
        formatted_texts = [self.format_text(text, text_type) for text in texts]
        
        self.logger.info(f"üîç –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤...")
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        try:
            embeddings = self.model.encode(
                formatted_texts,
                batch_size=batch_size,
                show_progress_bar=show_progress,
                convert_to_numpy=True,
                normalize_embeddings=True,
                device=self.device
            )
            
            self.logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
            return embeddings
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
            raise
    
    def embed_query(self, query: str) -> np.ndarray:
        """
        –°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.
        
        Args:
            query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        
        Returns:
            –≠–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
        """
        return self.embed_texts([query], text_type="query", show_progress=False)[0]
    
    def compute_similarity(self,
                          query_embedding: np.ndarray,
                          passage_embeddings: np.ndarray) -> np.ndarray:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–æ–º –∏ –ø–∞—Å—Å–∞–∂–∞–º–∏.
        
        Args:
            query_embedding: –≠–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
            passage_embeddings: –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–∞—Å—Å–∞–∂–µ–π
        
        Returns:
            –ú–∞—Å—Å–∏–≤ —Å—Ö–æ–¥—Å—Ç–≤
        """
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ–∫—Ç–æ—Ä—ã (–µ—Å–ª–∏ –µ—â–µ –Ω–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã)
        query_norm = query_embedding / np.linalg.norm(query_embedding)
        passages_norm = passage_embeddings / np.linalg.norm(passage_embeddings, axis=1, keepdims=True)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        similarities = np.dot(passages_norm, query_norm)
        return similarities
```


```python
class ChromaStorage:
    """
    –ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ–º ChromaDB.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç multilingual-e5-large –º–æ–¥–µ–ª—å.
    """
    
    def __init__(self,
                db_path: str,
                collection_name: str = Config.COLLECTION_NAME,
                embedding_model: MultilingualE5Embedder = None,
                create_if_missing: bool = True,
                reset_db: bool = False):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞.
        
        Args:
            db_path: –ü—É—Ç—å –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
            collection_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            embedding_model: –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            create_if_missing: –°–æ–∑–¥–∞–≤–∞—Ç—å –ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—é, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
            reset_db: –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –ª–∏ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö (–¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—à–∏–±–æ–∫)
        """
        self.db_path = Path(db_path)
        self.collection_name = collection_name
        self.create_if_missing = create_if_missing
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        if embedding_model is None:
            self.embedding_model = MultilingualE5Embedder()
        else:
            self.embedding_model = embedding_model
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∏–µ–Ω—Ç–∞ ChromaDB
        self.client = None
        self.collection = None
        self.vector_store = None
        self.logger = logging.getLogger(__name__)
        
        # –ü–µ—Ä–µ—Å–æ–∑–¥–∞—ë–º –±–∞–∑—É, –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è
        if reset_db and self.db_path.exists():
            self.logger.warning(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ–≤—Ä–µ–∂–¥—ë–Ω–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {self.db_path}")
            try:
                shutil.rmtree(self.db_path)
                self.logger.info("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —É–¥–∞–ª–µ–Ω–∞")
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –±–∞–∑—ã: {e}")
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        self.db_path.mkdir(parents=True, exist_ok=True)
        
        self._init_chroma()
    
    def _init_chroma(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç ChromaDB."""
        try:
            # ‚úÖ –ù–û–í–´–ô API ChromaDB (–≤–µ—Ä—Å–∏—è >= 1.0.0)
            self.client = chromadb.PersistentClient(
                path=str(self.db_path)
            )
            
            self.logger.info(f"‚úÖ ChromaDB –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {self.db_path}")
            
            # –°–æ–∑–¥–∞–µ–º –∏–ª–∏ –ø–æ–ª—É—á–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
            self._init_collection()
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ChromaDB: {e}")
            raise
    
    def _init_collection(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏—é."""
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é
            try:
                self.collection = self.client.get_collection(self.collection_name)
                self.logger.info(f"üìÇ –ö–æ–ª–ª–µ–∫—Ü–∏—è '{self.collection_name}' –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            except Exception:
                if self.create_if_missing:
                    # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ù–æ–≤—ã–π —Å–ø–æ—Å–æ–± —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏
                    self.collection = self.client.create_collection(
                        name=self.collection_name,
                        metadata={"hnsw:space": "cosine"}
                    )
                    self.logger.info(f"üÜï –°–æ–∑–¥–∞–Ω–∞ –∫–æ–ª–ª–µ–∫—Ü–∏—è '{self.collection_name}'")
                else:
                    raise ValueError(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è '{self.collection_name}' –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            
            # –°–æ–∑–¥–∞–µ–º LangChain –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
            self.vector_store = Chroma(
                client=self.client,
                collection_name=self.collection_name,
                embedding_function=self._get_langchain_embedding_function()
            )
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {e}")
            raise
    
    def _get_langchain_embedding_function(self):
        """–°–æ–∑–¥–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è LangChain."""
        # –°–æ–∑–¥–∞–µ–º –æ–±–µ—Ä—Ç–∫—É –¥–ª—è –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏
        class E5EmbeddingFunction:
            def __init__(self, embedder: MultilingualE5Embedder):
                self.embedder = embedder
            
            def embed_documents(self, texts: List[str]) -> List[List[float]]:
                embeddings = self.embedder.embed_texts(texts, text_type="passage")
                return embeddings.tolist()
            
            def embed_query(self, text: str) -> List[float]:
                embedding = self.embedder.embed_query(text)
                return embedding.tolist()
        
        return E5EmbeddingFunction(self.embedding_model)
    
    def add_texts(self,
                 texts: List[str],
                 metadatas: List[Dict] = None,
                 ids: List[str] = None,
                 batch_size: int = 32) -> List[str]:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —Ç–µ–∫—Å—Ç—ã –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ.
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            metadatas: –°–ø–∏—Å–æ–∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            ids: –°–ø–∏—Å–æ–∫ ID (–≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã)
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        
        Returns:
            –°–ø–∏—Å–æ–∫ ID –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
        """
        if not texts:
            self.logger.warning("‚ö†Ô∏è –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –ø—É—Å—Ç")
            return []
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ID, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã
        if ids is None:
            ids = [f"doc_{hashlib.md5(text.encode()).hexdigest()[:12]}" 
                  for text in texts]
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        if metadatas is None:
            metadatas = [{} for _ in texts]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É —Å–ø–∏—Å–∫–æ–≤
        if len(texts) != len(ids) or len(texts) != len(metadatas):
            raise ValueError("–î–ª–∏–Ω—ã texts, ids –∏ metadatas –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å")
        
        self.logger.info(f"‚ûï –î–æ–±–∞–≤–ª–µ–Ω–∏–µ {len(texts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –±–∞—Ç—á–∞–º–∏
        added_ids = []
        for i in tqdm(range(0, len(texts), batch_size), desc="–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ ChromaDB"):
            batch_texts = texts[i:i+batch_size]
            batch_ids = ids[i:i+batch_size]
            batch_metadatas = metadatas[i:i+batch_size]
            
            # ‚úÖ –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            batch_embeddings = self.embedding_model.embed_texts(
                batch_texts, 
                text_type="passage",
                show_progress=False
            )
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é
            self.collection.add(
                documents=batch_texts,
                embeddings=batch_embeddings.tolist(),
                metadatas=batch_metadatas,
                ids=batch_ids
            )
            added_ids.extend(batch_ids)
        
        self.logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(added_ids)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        return added_ids
    
    def search(self,
              query: str,
              top_k: int = Config.DEFAULT_TOP_K,
              score_threshold: float = Config.SIMILARITY_THRESHOLD,
              filter_meta: Dict[str, Any] = None) -> List[SearchResult]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫.
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            score_threshold: –ü–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞
            filter_meta: –§–∏–ª—å—Ç—Ä –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        """
        if not query.strip():
            raise ValueError("–ó–∞–ø—Ä–æ—Å –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")
        
        self.logger.info(f"üîç –ü–æ–∏—Å–∫: '{query[:50]}...'")
        
        try:
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self.embedding_model.embed_query(query)
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
            results = self.collection.query(
                query_embeddings=[query_embedding.tolist()],
                n_results=top_k,
                where=filter_meta
            )
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            search_results = []
            if results['ids'] and len(results['ids'][0]) > 0:
                for i in range(len(results['ids'][0])):
                    # –ü–æ–ª—É—á–∞–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ö–æ–¥—Å—Ç–≤–æ
                    distance = results['distances'][0][i] if 'distances' in results else 0
                    similarity = 1.0 / (1.0 + distance)  # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ distance –≤ similarity
                    
                    if similarity >= score_threshold:
                        result = SearchResult(
                            id=results['ids'][0][i],
                            content=results['documents'][0][i],
                            similarity_score=similarity,
                            metadata=results['metadatas'][0][i] if results['metadatas'] else {}
                        )
                        search_results.append(result)
            
            self.logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(search_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return search_results
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            raise
    
    def delete_by_ids(self, ids: List[str]) -> int:
        """
        –£–¥–∞–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ ID.
        
        Args:
            ids: –°–ø–∏—Å–æ–∫ ID –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
        
        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        if not ids:
            return 0
        
        try:
            self.collection.delete(ids=ids)
            self.logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ {len(ids)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            return len(ids)
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è: {e}")
            raise
    
    def get_collection_info(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–ª–ª–µ–∫—Ü–∏–∏."""
        try:
            count = self.collection.count()
            # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            collection_metadata = self.collection.metadata or {}
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–∏–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            sample = self.collection.peek() if count > 0 else {}
            
            return {
                "collection_name": self.collection_name,
                "document_count": count,
                "embedding_dimension": self.embedding_model.get_embedding_dimension(),
                "db_path": str(self.db_path),
                "metadata": collection_metadata,
                "sample_size": len(sample.get("documents", [])),
                "model": self.embedding_model.model_name
            }
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: {e}")
            return {}
    
    def clear_collection(self) -> None:
        """–û—á–∏—â–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏—é."""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ ID
            all_data = self.collection.get()
            if all_data['ids']:
                self.delete_by_ids(all_data['ids'])
            self.logger.info(f"üßπ –ö–æ–ª–ª–µ–∫—Ü–∏—è '{self.collection_name}' –æ—á–∏—â–µ–Ω–∞")
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {e}")
```


```python
class RAGPipeline:
    """
    üöÄ –ü–æ–ª–Ω–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π RAG –ø–∞–π–ø–ª–∞–π–Ω —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π:
    - –í–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ ChromaDB (–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ ChromaStorage)
    - LLM –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤
    - RAGAS –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    - –î–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    """
    
    def __init__(self,
                 data_dir: str = None,
                 db_path: str = None,
                 config: Config = None,
                 device: str = None,
                 llm_generator: LLMGenerator = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞.
        
        Args:
            data_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏
            db_path: –ü—É—Ç—å –∫ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π (cuda/cpu)
            llm_generator: –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ—Ç–≤–µ—Ç–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        self.config = config or Config
        self.data_dir = Path(data_dir or self.config.DEFAULT_DATA_DIR)
        self.db_path = Path(db_path or self.config.DEFAULT_DB_DIR)
        
        # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ
        if device:
            self.config.DEVICE = device
            self.config.USE_FP16 = True if device == "cuda" else False
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.config.setup_directories()
        JupyterConfig.setup_directories()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.data_loader = DataLoader(self.data_dir)
        self.embedder = MultilingualE5Embedder(
            device=self.config.DEVICE, 
            use_fp16=self.config.USE_FP16
        )
        
        self.storage = None  
        
        self.llm_generator = llm_generator
        
        self.logger = logging.getLogger(__name__)
        self.setup_logging()
        
        self.logger.info(f"üöÄ RAG Pipeline –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {self.config.DEVICE}")
    
    def setup_logging(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ."""
        log_file = Path(self.config.DEFAULT_LOGS_DIR) / f"rag_pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        # –£–¥–∞–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
        for handler in logging.root.handlers[:]:
            logging.root.removeHandler(handler)
        
        logging.basicConfig(
            level=getattr(logging, self.config.LOG_LEVEL),
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
    
    # ========== –ú–ï–¢–û–î–´ –†–ê–ë–û–¢–´ –° –î–ê–ù–ù–´–ú–ò ==========
    
    def load_and_process_data(self,
                            file_pattern: str = "*.txt",
                            max_files: int = None,
                            save_intermediate: bool = True) -> pd.DataFrame:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ.
        
        Args:
            file_pattern: –®–∞–±–ª–æ–Ω —Ñ–∞–π–ª–æ–≤
            max_files: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤
            save_intermediate: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        
        Returns:
            DataFrame —Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        """
        self.logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        df = self.data_loader.load_files_to_dataframe(
            file_pattern=file_pattern,
            max_files=max_files,
            process_chunks=True
        )
        
        if df.empty:
            self.logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
            return df
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if save_intermediate:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = Path(JupyterConfig.RESULTS_DIR) / f"processed_data_{timestamp}.parquet"
            self.data_loader.save_dataframe(df, output_path)
            self.logger.info(f"üíæ –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")
        
        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞–Ω–Ω—ã—Ö
        display_df = df.head(JupyterConfig.DISPLAY_TABLE_ROWS).copy()
        display_df['text_preview'] = display_df['text'].apply(
            lambda x: x[:JupyterConfig.DISPLAY_MAX_CHARS] + '...' if len(x) > JupyterConfig.DISPLAY_MAX_CHARS else x
        )
        
        print(f"\nüìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —á–∞–Ω–∫–æ–≤ –∏–∑ {df['source_file'].nunique()} —Ñ–∞–π–ª–æ–≤")
        print(f"üî§ –ü—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:")
        display(display_df[['id', 'source_file', 'chunk_index', 'total_chunks', 'text_preview']])
        
        return df
    
    def initialize_storage(self, collection_name: str = None) -> ChromaStorage:
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ.
        
        Args:
            collection_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        
        Returns:
            –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        """
        self.logger.info("üì¶ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞...")
        
        # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º ChromaStorage –∫–ª–∞—Å—Å –∏–∑ —Å—Ç–∞—Ä–æ–π –≤–µ—Ä—Å–∏–∏
        self.storage = ChromaStorage(
            db_path=str(self.db_path),
            collection_name=collection_name or self.config.COLLECTION_NAME,
            embedding_model=self.embedder
        )
        
        info = self.storage.get_collection_info()
        self.logger.info(f"‚úÖ –•—Ä–∞–Ω–∏–ª–∏—â–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {info}")
        
        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        html = f"""
        <div style="border: 1px solid #ddd; border-radius: 8px; padding: 15px; margin: 10px 0; background-color: #f8f9fa;">
            <h3 style="color: #2196F3; margin-top: 0;">üì¶ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ</h3>
            <ul style="line-height: 1.6;">
                <li><strong>–ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏:</strong> {info['collection_name']}</li>
                <li><strong>–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:</strong> {info['document_count']}</li>
                <li><strong>–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤:</strong> {info['embedding_dimension']}</li>
                <li><strong>–ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤:</strong> {info['model']}</li>
                <li><strong>–ü—É—Ç—å –∫ –±–∞–∑–µ:</strong> {info['db_path']}</li>
            </ul>
        </div>
        """
        display(HTML(html))
        
        return self.storage
    
    def build_vector_store(self,
                         df: pd.DataFrame,
                         clear_existing: bool = False) -> List[str]:
        """
        –°—Ç—Ä–æ–∏—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∏–∑ DataFrame.
        
        Args:
            df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
            clear_existing: –û—á–∏—Å—Ç–∏—Ç—å –ª–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é
        
        Returns:
            –°–ø–∏—Å–æ–∫ ID –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        if self.storage is None:
            self.initialize_storage()
        
        if df.empty:
            self.logger.error("‚ùå DataFrame –ø—É—Å—Ç")
            return []
        
        # –û—á–∏—â–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if clear_existing:
            self.logger.info("üßπ –û—á–∏—Å—Ç–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏...")
            self.storage.clear_collection()
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        texts = df["text"].tolist()
        ids = df["id"].tolist()
        
        # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadatas = []
        for _, row in df.iterrows():
            metadata = {k: v for k, v in row.items() if k not in ["id", "text"]}
            metadata["id"] = row["id"]
            metadatas.append(metadata)
        
        self.logger.info(f"üöÄ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ {len(texts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ...")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        added_ids = self.storage.add_texts(
            texts=texts,
            metadatas=metadatas,
            ids=ids,
            batch_size=self.config.BATCH_SIZE
        )
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        info = self.storage.get_collection_info()
        self.logger.info(f"‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–æ: {info}")
        
        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        html = f"""
        <div style="border: 2px solid #4CAF50; border-radius: 8px; padding: 15px; margin: 10px 0; background-color: #e8f5e9;">
            <h3 style="color: #4CAF50; margin-top: 0;">‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —É—Å–ø–µ—à–Ω–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–æ!</h3>
            <ul style="line-height: 1.6;">
                <li><strong>–î–æ–±–∞–≤–ª–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:</strong> {len(added_ids)}</li>
                <li><strong>–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏:</strong> {info['document_count']}</li>
                <li><strong>–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤:</strong> {info['embedding_dimension']}</li>
                <li><strong>–ú–æ–¥–µ–ª—å:</strong> {info['model']}</li>
            </ul>
        </div>
        """
        display(HTML(html))
        
        return added_ids
    
    # ========== –ú–ï–¢–û–î–´ –ü–û–ò–°–ö–ê ==========
    
    def search(self,
              query: str,
              top_k: int = None,
              score_threshold: float = None,
              return_df: bool = False) -> Union[List[SearchResult], pd.DataFrame]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ.
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            score_threshold: –ü–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞
            return_df: –í–µ—Ä–Ω—É—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –≤–∏–¥–µ DataFrame
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –∫–∞–∫ —Å–ø–∏—Å–æ–∫ SearchResult –∏–ª–∏ DataFrame
        """
        if self.storage is None:
            self.logger.error("‚ùå –•—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ")
            return [] if not return_df else pd.DataFrame()
        
        top_k = top_k or self.config.DEFAULT_TOP_K
        score_threshold = score_threshold or self.config.SIMILARITY_THRESHOLD
        
        self.logger.info(f"üîç –ü–æ–∏—Å–∫ –∑–∞–ø—Ä–æ—Å–∞: '{query}'")
        
        # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–æ–¥ search –∏–∑ ChromaStorage
        results = self.storage.search(
            query=query,
            top_k=top_k,
            score_threshold=score_threshold
        )
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if results:
            self.logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
            for i, result in enumerate(results[:3]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3
                self.logger.info(f"  {i+1}. Score: {result.similarity_score:.4f}, Source: {result.metadata.get('source_file', 'unknown')}")
        else:
            self.logger.info("‚ö†Ô∏è –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        
        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        if results:
            self._display_search_results(query, results, score_threshold)
        
        if return_df:
            return self.results_to_dataframe(results)
        
        return results
    
    def _display_search_results(self, query: str, results: List[SearchResult], score_threshold: float):
        """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ."""
        html = f"""
        <div style="border: 1px solid #2196F3; border-radius: 8px; padding: 15px; margin: 10px 0;">
            <h3 style="color: #2196F3; margin-top: 0;">üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –¥–ª—è: "{query}"</h3>
            <p style="color: #666;"><strong>–ù–∞–π–¥–µ–Ω–æ:</strong> {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ | <strong>–ü–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞:</strong> {score_threshold}</p>
        """
        
        for i, result in enumerate(results):
            color = '#e8f5e9' if result.similarity_score > 0.8 else '#fff8e1' if result.similarity_score > 0.6 else '#ffebee'
            border_color = '#4CAF50' if result.similarity_score > 0.8 else '#FFA726' if result.similarity_score > 0.6 else '#EF5350'
            
            html += f"""
            <div style="border: 2px solid {border_color}; border-radius: 8px; padding: 15px; margin: 10px 0; background-color: {color};">
                <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 10px;">
                    <strong style="font-size: 1.1em; color: #1a237e;">–†–µ–∑—É–ª—å—Ç–∞—Ç #{i+1}</strong>
                    <span style="background-color: {border_color}; color: white; padding: 3px 8px; border-radius: 12px; font-weight: bold;">
                        Score: {result.similarity_score:.4f}
                    </span>
                </div>
                <div style="background-color: white; padding: 10px; border-radius: 5px; margin: 5px 0; border-left: 3px solid {border_color};">
                    {result.content[:300]}...
                </div>
                <div style="color: #666; font-size: 0.9em; margin-top: 8px;">
                    <strong>–ò—Å—Ç–æ—á–Ω–∏–∫:</strong> {result.metadata.get('source_file', 'unknown')} | 
                    <strong>–ß–∞–Ω–∫:</strong> {result.metadata.get('chunk_index', 'N/A')}/{result.metadata.get('total_chunks', 'N/A')}
                </div>
            </div>
            """
        
        html += "</div>"
        display(HTML(html))
    
    def results_to_dataframe(self, results: List[SearchResult]) -> pd.DataFrame:
        """
        –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –≤ DataFrame.
        
        Args:
            results: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        
        Returns:
            DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        """
        if not results:
            return pd.DataFrame()
        
        data = []
        for result in results:
            row = {
                'id': result.id,
                'content': result.content,
                'similarity_score': result.similarity_score,
                'source_file': result.metadata.get('source_file', ''),
                'chunk_index': result.metadata.get('chunk_index', ''),
                'total_chunks': result.metadata.get('total_chunks', ''),
            }
            data.append(row)
        
        df = pd.DataFrame(data)
        return df
    
    # ========== LLM –ì–ï–ù–ï–†–ê–¶–ò–Ø ==========
    
    def set_llm_generator(self, llm_generator: LLMGenerator):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç LLM –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä."""
        self.llm_generator = llm_generator
        self.logger.info("‚úÖ LLM –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    
    def query(self,
             query: str,
             top_k: int = None,
             score_threshold: float = None,
             return_details: bool = False,
             display_result: bool = True) -> Union[str, Dict]:
        """
        ‚ú® –ü–æ–ª–Ω—ã–π RAG –∑–∞–ø—Ä–æ—Å: –ø–æ–∏—Å–∫ + –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞.
        
        Args:
            query: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
            score_threshold: –ü–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞
            return_details: –í–æ–∑–≤—Ä–∞—â–∞—Ç—å –¥–µ—Ç–∞–ª–∏ (–∫–æ–Ω—Ç–µ–∫—Å—Ç—ã, –∏—Å—Ç–æ—á–Ω–∏–∫–∏)
            display_result: –û—Ç–æ–±—Ä–∞–∂–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        
        Returns:
            –û—Ç–≤–µ—Ç –∏–ª–∏ —Å–ª–æ–≤–∞—Ä—å —Å –¥–µ—Ç–∞–ª—è–º–∏
        """
        if self.llm_generator is None:
            raise ValueError("‚ùå LLM –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ set_llm_generator()")
        
        if self.storage is None:
            raise ValueError("‚ùå –•—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ initialize_storage()")
        
        top_k = top_k or self.config.DEFAULT_TOP_K
        score_threshold = score_threshold or self.config.SIMILARITY_THRESHOLD
        
        self.logger.info(f"üí¨ RAG –∑–∞–ø—Ä–æ—Å: '{query}'")
        
        # 1. –ü–æ–∏—Å–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤
        search_results = self.search(query, top_k=top_k, score_threshold=score_threshold)
        
        if not search_results:
            answer = "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å."
            if display_result:
                self._display_query_result(query, answer, [], [], [])
            if return_details:
                return {"answer": answer, "contexts": [], "sources": [], "scores": []}
            return answer
        
        # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã
        contexts = [r.content for r in search_results]
        sources = [r.metadata.get('source_file', 'unknown') for r in search_results]
        scores = [r.similarity_score for r in search_results]
        
        # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        self.logger.info("ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞...")
        answer = self.llm_generator.generate_answer(query, contexts)
        
        # 4. –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if display_result:
            self._display_query_result(query, answer, contexts, sources, scores)
        
        if return_details:
            return {
                "answer": answer,
                "contexts": contexts,
                "sources": sources,
                "scores": scores,
                "search_results": search_results
            }
        
        return answer
    
    def _display_query_result(self, query: str, answer: str, contexts: List[str], 
                             sources: List[str], scores: List[float]):
        """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç RAG –∑–∞–ø—Ä–æ—Å–∞ –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ."""
        html = f"""
        <div style="border: 2px solid #673AB7; border-radius: 12px; padding: 20px; margin: 15px 0; background: linear-gradient(135deg, #f3e5f5 0%, #e1bee7 100%);">
            <h2 style="color: #4A148C; margin-top: 0;">üí¨ RAG –û—Ç–≤–µ—Ç</h2>
            
            <div style="background: white; padding: 15px; border-radius: 8px; margin: 15px 0; border-left: 5px solid #673AB7;">
                <strong style="color: #666; display: block; margin-bottom: 8px;">‚ùì –í–æ–ø—Ä–æ—Å:</strong>
                <div style="color: #1a237e; font-size: 1.1em;">{query}</div>
            </div>
            
            <div style="background: #e8f5e9; padding: 15px; border-radius: 8px; margin: 15px 0; border-left: 5px solid #4CAF50;">
                <strong style="color: #666; display: block; margin-bottom: 8px;">‚úÖ –û—Ç–≤–µ—Ç:</strong>
                <div style="color: #1b5e20; line-height: 1.6;">{answer}</div>
            </div>
        """
        
        if contexts:
            html += f"""
            <div style="margin-top: 20px;">
                <h3 style="color: #4A148C;">üìö –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã ({len(contexts)}):</h3>
            """
            
            for i, (ctx, src, score) in enumerate(zip(contexts, sources, scores)):
                color = '#4CAF50' if score > 0.8 else '#FFA726' if score > 0.6 else '#EF5350'
                html += f"""
                <div style="background: white; padding: 12px; margin: 8px 0; border-radius: 6px; border-left: 4px solid {color};">
                    <div style="display: flex; justify-content: space-between; margin-bottom: 5px;">
                        <strong style="color: #666;">–ö–æ–Ω—Ç–µ–∫—Å—Ç #{i+1}</strong>
                        <span style="background: {color}; color: white; padding: 2px 8px; border-radius: 10px; font-size: 0.9em;">
                            Score: {score:.4f}
                        </span>
                    </div>
                    <div style="color: #333; font-size: 0.95em; margin: 8px 0;">{ctx[:200]}...</div>
                    <div style="color: #999; font-size: 0.85em;">üìÑ –ò—Å—Ç–æ—á–Ω–∏–∫: {src}</div>
                </div>
                """
            
            html += "</div>"
        
        html += "</div>"
        display(HTML(html))
    
    # ========== –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê ==========
    
    def evaluate_search(self,
                       test_queries: List[str],
                       expected_results: List[List[str]] = None,
                       display_results: bool = True) -> Dict[str, Any]:
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞.
        
        Args:
            test_queries: –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            expected_results: –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            display_results: –û—Ç–æ–±—Ä–∞–∂–∞—Ç—å –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ
        
        Returns:
            –ú–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏
        """
        self.logger.info(f"üß™ –û—Ü–µ–Ω–∫–∞ –ø–æ–∏—Å–∫–∞ –Ω–∞ {len(test_queries)} –∑–∞–ø—Ä–æ—Å–∞—Ö...")
        
        metrics = {
            "total_queries": len(test_queries),
            "results_per_query": [],
            "avg_similarity_score": [],
            "execution_times": [],
            "successful_queries": 0
        }
        
        evaluation_results = []
        
        for query in tqdm(test_queries, desc="–û—Ü–µ–Ω–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤"):
            start_time = datetime.now()
            try:
                results = self.search(query, top_k=5, return_df=False)
                execution_time = (datetime.now() - start_time).total_seconds()
                
                metrics["results_per_query"].append(len(results))
                metrics["execution_times"].append(execution_time)
                
                if results:
                    metrics["successful_queries"] += 1
                    avg_similarity = np.mean([r.similarity_score for r in results])
                    metrics["avg_similarity_score"].append(avg_similarity)
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                    eval_result = {
                        'query': query,
                        'results_count': len(results),
                        'avg_similarity': avg_similarity,
                        'execution_time': execution_time,
                        'top_result': results[0].content[:100] + '...' if results else None
                    }
                    evaluation_results.append(eval_result)
                
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ '{query}': {e}")
                metrics["results_per_query"].append(0)
                metrics["execution_times"].append(0)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        metrics["avg_results_per_query"] = np.mean(metrics["results_per_query"]) if metrics["results_per_query"] else 0
        metrics["avg_execution_time"] = np.mean(metrics["execution_times"]) if metrics["execution_times"] else 0
        metrics["success_rate"] = metrics["successful_queries"] / metrics["total_queries"] if metrics["total_queries"] > 0 else 0
        
        if metrics["avg_similarity_score"]:
            metrics["avg_similarity"] = np.mean(metrics["avg_similarity_score"])
        
        self.logger.info(f"‚úÖ –û—Ü–µ–Ω–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {metrics}")
        
        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        if display_results:
            self.display_evaluation_metrics(metrics, evaluation_results)
        
        return metrics
    
    def display_evaluation_metrics(self, metrics: Dict[str, Any], evaluation_results: List[Dict]):
        """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏ –≤ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ."""
        html = f"""
        <div style="border: 2px solid #2196F3; border-radius: 8px; padding: 20px; margin: 15px 0; background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%);">
            <h2 style="color: #1565C0; margin-top: 0; text-align: center;">üìä –ú–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏ –ø–æ–∏—Å–∫–∞</h2>
            
            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; margin: 20px 0;">
        """
        
        metric_items = [
            ('–í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤', metrics['total_queries'], '#1565C0'),
            ('–£—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤', metrics['successful_queries'], '#4CAF50'),
            ('–°–∫–æ—Ä–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞', f"{metrics['success_rate']:.1%}", '#FFA726'),
            ('–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è', f"{metrics['avg_execution_time']:.3f} —Å–µ–∫", '#9C27B0'),
            ('–†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –∑–∞–ø—Ä–æ—Å', f"{metrics['avg_results_per_query']:.1f}", '#009688'),
            ('–°—Ä–µ–¥–Ω–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ', f"{metrics.get('avg_similarity', 0):.4f}", '#E91E63')
        ]
        
        for title, value, color in metric_items:
            html += f"""
            <div style="text-align: center; padding: 15px; background: {color}15; border-radius: 8px; border: 1px solid {color}50;">
                <div style="font-size: 2.5em; color: {color}; margin-bottom: 8px;">{value}</div>
                <div style="font-weight: 500; color: #333;">{title}</div>
            </div>
            """
        
        html += """
            </div>
            
            <h3 style="color: #1565C0; margin-top: 25px;">üìà –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º:</h3>
            <div style="margin-top: 15px;">
        """
        
        for i, result in enumerate(evaluation_results[:10]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 10
            success_color = '#4CAF50' if result['results_count'] > 0 else '#EF5350'
            html += f"""
            <div style="background: white; padding: 12px; margin: 8px 0; border-radius: 6px; border-left: 4px solid {success_color};">
                <div style="display: flex; justify-content: space-between;">
                    <strong style="color: #1a237e;">–ó–∞–ø—Ä–æ—Å #{i+1}:</strong>
                    <span style="color: {success_color}; font-weight: bold;">{result['results_count']} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤</span>
                </div>
                <div style="color: #666; margin: 5px 0; font-style: italic;">"{result['query']}"</div>
                <div style="display: flex; justify-content: space-between; font-size: 0.9em; color: #666;">
                    <span>‚è∞ –í—Ä–µ–º—è: {result['execution_time']:.3f} —Å–µ–∫</span>
                    <span>‚≠ê –°—Ö–æ–¥—Å—Ç–≤–æ: {result['avg_similarity']:.4f}</span>
                </div>
                <div style="color: #666; margin-top: 5px; font-size: 0.9em;">
                    <strong>–¢–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç:</strong> {result['top_result'] or '–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤'}
                </div>
            </div>
            """
        
        html += """
            </div>
        </div>
        """
        
        display(HTML(html))
    
    # ========== –≠–ö–°–ü–û–†–¢ –ò –°–û–•–†–ê–ù–ï–ù–ò–ï ==========
    
    def save_pipeline_state(self, filename: str = None):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞.
        
        Args:
            filename: –ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"rag_pipeline_state_{timestamp}.pkl"
        
        filepath = Path(JupyterConfig.RESULTS_DIR) / filename
        
        state = {
            'config': asdict(self.config),
            'data_dir': str(self.data_dir),
            'db_path': str(self.db_path),
            'collection_info': self.storage.get_collection_info() if self.storage else None,
            'has_llm': self.llm_generator is not None,
            'timestamp': datetime.now().isoformat()
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(state, f)
        
        print(f"üíæ –°–æ—Å—Ç–æ—è–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filepath}")
        return filepath
    
    def export_results(self,
                     results: List[SearchResult],
                     output_format: str = "json",
                     filename: str = None) -> str:
        """
        –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞.
        
        Args:
            results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
            output_format: –§–æ—Ä–º–∞—Ç —ç–∫—Å–ø–æ—Ä—Ç–∞ (json/csv/html)
            filename: –ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        
        Returns:
            –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
        """
        if not results:
            self.logger.warning("‚ö†Ô∏è –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
            return ""
        
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"search_results_{timestamp}"
        
        filepath = Path(JupyterConfig.RESULTS_DIR) / filename
        
        if output_format == "json":
            result_dicts = [r.to_dict() for r in results]
            filepath = filepath.with_suffix('.json')
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(result_dicts, f, ensure_ascii=False, indent=2)
            print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ JSON: {filepath}")
            return str(filepath)
            
        elif output_format == "csv":
            df = self.results_to_dataframe(results)
            filepath = filepath.with_suffix('.csv')
            df.to_csv(filepath, index=False, encoding='utf-8')
            print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ CSV: {filepath}")
            return str(filepath)
            
        elif output_format == "html":
            html_content = self.generate_html_report(results)
            filepath = filepath.with_suffix('.html')
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(html_content)
            print(f"üíæ HTML –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {filepath}")
            return str(filepath)
            
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {output_format}")
    
    def generate_html_report(self, results: List[SearchResult]) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç HTML –æ—Ç—á–µ—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–æ–∏—Å–∫–∞."""
        html = """
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="utf-8">
            <title>RAG Search Results</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }
                .container { max-width: 1200px; margin: 0 auto; background: white; padding: 20px; border-radius: 10px; box-shadow: 0 1px 3px rgba(0,0,0,0.12); }
                .header { text-align: center; color: #1a237e; margin-bottom: 30px; }
                .result { border: 1px solid #ddd; border-radius: 8px; padding: 15px; margin: 15px 0; background: #f8f9fa; }
                .score { background: #4CAF50; color: white; padding: 3px 8px; border-radius: 12px; font-weight: bold; float: right; }
                .content { background: white; padding: 10px; margin: 10px 0; border-left: 3px solid #2196F3; }
                .metadata { color: #666; font-size: 0.9em; margin-top: 8px; }
                .source { color: #1565C0; font-weight: bold; }
            </style>
        </head>
        <body>
            <div class="container">
                <div class="header">
                    <h1>üîç RAG Search Results</h1>
                    <p>Generated on """ + datetime.now().strftime("%Y-%m-%d %H:%M:%S") + """</p>
                </div>
        """
        
        for i, result in enumerate(results):
            html += f"""
                <div class="result">
                    <div style="display: flex; justify-content: space-between; align-items: center;">
                        <h3>Result #{i+1}</h3>
                        <span class="score">Score: {result.similarity_score:.4f}</span>
                    </div>
                    <div class="content">
                        {result.content.replace('\n', '<br>')}
                    </div>
                    <div class="metadata">
                        <span class="source">Source: {result.metadata.get('source_file', 'unknown')}</span> | 
                        Chunk: {result.metadata.get('chunk_index', 'N/A')}/{result.metadata.get('total_chunks', 'N/A')} |
                        ID: {result.id[:12]}...
                    </div>
                </div>
            """
        
        html += """
            </div>
        </body>
        </html>
        """
        return html
```


```python
def get_device_info():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ."""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    return {
        "device": device,
        "gpu_available": torch.cuda.is_available(),
        "gpu_name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
        "gpu_memory": torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0,
        "cuda_version": torch.version.cuda if torch.cuda.is_available() else None
    }
```


```python
def initialize_rag_pipeline(
    data_dir: str = "data/chunks",
    db_path: str = "vector_db",
    device: str = None,
    setup_llm: bool = True,
    max_files: int = 1000,
    build_store: bool = False,
    reset_db: bool = False
) -> RAGPipeline:
    """
    –ü–æ–ª–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG –ø–∞–π–ø–ª–∞–π–Ω–∞.
    
    Args:
        data_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏
        db_path: –ü—É—Ç—å –∫ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ
        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (auto –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
        setup_llm: –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ª–∏ LLM –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä
        max_files: –ú–∞–∫—Å–∏–º—É–º —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        reset_db: –ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö (—Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å –ø–æ–≤—Ä–µ–∂–¥—ë–Ω–Ω–æ–π –ë–î)
    
    Returns:
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π RAG –ø–∞–π–ø–ª–∞–π–Ω
    """
    try:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"
        
        print(f"üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG –ø–∞–π–ø–ª–∞–π–Ω–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {device}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ
        device_info = get_device_info()
        print(f"\n‚ÑπÔ∏è  –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ:")
        print(f"   ‚Ä¢ –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device_info['device']}")
        if device_info['gpu_available']:
            print(f"   ‚Ä¢ GPU: {device_info['gpu_name']}")
            print(f"   ‚Ä¢ –ü–∞–º—è—Ç—å GPU: {device_info['gpu_memory']:.2f} GB")
            print(f"   ‚Ä¢ CUDA –≤–µ—Ä—Å–∏—è: {device_info['cuda_version']}")
        
        # ‚úÖ –ï—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è - —É–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é –±–∞–∑—É
        if reset_db:
            db_path_obj = Path(db_path)
            if db_path_obj.exists():
                print(f"\nüóëÔ∏è –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {db_path}")
                shutil.rmtree(db_path_obj)
                print("‚úÖ –°—Ç–∞—Ä–∞—è –±–∞–∑–∞ —É–¥–∞–ª–µ–Ω–∞")
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω
        pipeline = RAGPipeline(
            data_dir=data_dir,
            db_path=db_path,
            device=device
        )
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º LLM –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä
        if setup_llm:
            print("\nü§ñ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LLM –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞...")
            generation_client = UniversalLLMClient(
                model=Config.GENERATION_MODEL,
                api_key=Config.LLM_API_KEY,
                api_base=Config.LLM_API_BASE,
                temperature=Config.GENERATION_TEMPERATURE,
                max_tokens=Config.GENERATION_MAX_TOKENS
            )
            llm_generator = LLMGenerator(llm_client=generation_client)
            pipeline.set_llm_generator(llm_generator)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        print("\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        df = pipeline.load_and_process_data(
            file_pattern="*.txt",
            max_files=max_files,
            save_intermediate=True
        )
        
        if not df.empty:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
            print("\nüì¶ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞...")
            pipeline.initialize_storage()

            if build_store:
                # –°—Ç—Ä–æ–∏–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
                print("\nüöÄ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞...")
                pipeline.build_vector_store(df, clear_existing=True)
            
            print("\n‚úÖ –ü–∞–π–ø–ª–∞–π–Ω —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –∏ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")
            print("\nüìù –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–µ—Ç–æ–¥—ã:")
            print("   ‚Ä¢ pipeline.search(query) - –ø–æ–∏—Å–∫ –ø–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ")
            print("   ‚Ä¢ pipeline.query(query) - –ø–æ–ª–Ω—ã–π RAG –∑–∞–ø—Ä–æ—Å —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –æ—Ç–≤–µ—Ç–∞")
            print("   ‚Ä¢ pipeline.evaluate_search(queries) - –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞")
            
            return pipeline
        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
            return None
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞: {e}")
        import traceback
        traceback.print_exc()
        return None
```

## –ü—Ä–æ—Ö–æ–¥ –ø–æ –∫–æ–Ω–≤–µ–π–µ—Ä—É

### –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è


```python
import hashlib
```


```python
pipeline = initialize_rag_pipeline(
    data_dir="data/chunks",
    db_path="vector_db",
    setup_llm=True,
    max_files=1000,
    # build_store=True,
    # reset_db=True
)
```

### –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö


```python
test_cases = [
    {
        "query": "Tell me all about I101 and its relations in Attempto Controlled English",
        "ground_truth": """i101 is a length_measure_with_unit. It represents a measurement involving 
        a length and a unit. i101 has two key components: i101_value_component (the numerical value) 
        and i17 (the unit component). i101 also serves as a conversion factor for i103."""
    },
    {
        "query": "What is i103 and how does it relate to i101?",
        "ground_truth": """i103 is a conversion_based_unit that uses i101 as its conversion factor. 
        This means i101 provides the necessary factor for converting between i103 and other units."""
    },
    {
        "query": "Explain the relationship between i101, i17, and i101_value_component",
        "ground_truth": """i101 is a length_measure_with_unit that consists of two parts: 
        i17 (unit component) and i101_value_component (value component). The measure i101 
        combines these two to represent a complete length measurement."""
    }
]
```


```python
test_queries = [test_cases[0]['query'], test_cases[1]['query'], test_cases[2]['query']]
test_queries
```




    ['Tell me all about I101 and its relations in Attempto Controlled English',
     'What is i103 and how does it relate to i101?',
     'Explain the relationship between i101, i17, and i101_value_component']



### –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ —á–µ—Ä–µ–∑ RAG


```python
question = "Tell me all about I101 and its relations in Attempto Controlled English"
```


```python
if pipeline:
    # 1. –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫
    results = pipeline.search(question, 
                              top_k=5)
```


```python
# 2. –ü–æ–ª–Ω—ã–π RAG –∑–∞–ø—Ä–æ—Å
answer = pipeline.query(question, display_result=True)
```


```python
# 3. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
metrics = pipeline.evaluate_search(test_queries)
```

## –ú–µ—Ç—Ä–∏–∫–∏ RAGAS


```python
def prepare_rag_evaluation_dataset(
    pipeline,  # RAGPipeline
    test_cases: List[Dict[str, str]],
    top_k: int = 5
) -> Tuple[List[str], List[str], List[List[str]], List[str]]:
    """
    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ RAGAS, –ø–æ–ª—É—á–∞—è –æ—Ç–≤–µ—Ç—ã –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã –æ—Ç RAG —Å–∏—Å—Ç–µ–º—ã.
    
    Args:
        pipeline: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π RAG –ø–∞–π–ø–ª–∞–π–Ω
        test_cases: –°–ø–∏—Å–æ–∫ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∫–µ–π—Å–æ–≤ —Å query –∏ ground_truth
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
    
    Returns:
        –ö–æ—Ä—Ç–µ–∂ (queries, answers, contexts_list, ground_truths)
    """
    queries = []
    answers = []
    contexts_list = []
    ground_truths = []
    
    print(f"üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è {len(test_cases)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤...")
    
    for test_case in tqdm(test_cases, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤"):
        query = test_case["query"]
        ground_truth = test_case.get("ground_truth", "")
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã –æ—Ç RAG —Å–∏—Å—Ç–µ–º—ã
        try:
            result = pipeline.query(
                query=query,
                top_k=top_k,
                return_details=True,
                display_result=False  # –ù–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
            queries.append(query)
            answers.append(result["answer"])
            contexts_list.append(result["contexts"])
            ground_truths.append(ground_truth)
            
            print(f"‚úÖ –ó–∞–ø—Ä–æ—Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {query[:50]}...")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞ '{query[:50]}...': {e}")
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, —á—Ç–æ–±—ã –Ω–µ –Ω–∞—Ä—É—à–∏—Ç—å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é
            queries.append(query)
            answers.append("")
            contexts_list.append([])
            ground_truths.append(ground_truth)
    
    print(f"\n‚úÖ –î–∞—Ç–∞—Å–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω:")
    print(f"   ‚Ä¢ –ó–∞–ø—Ä–æ—Å–æ–≤: {len(queries)}")
    print(f"   ‚Ä¢ –° –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º–∏: {sum(1 for c in contexts_list if c)}")
    print(f"   ‚Ä¢ –° ground truth: {sum(1 for gt in ground_truths if gt)}")
    
    return queries, answers, contexts_list, ground_truths
```


```python
def evaluate_rag_with_ragas(
    pipeline,  # RAGPipeline
    test_cases: List[Dict[str, str]],
    judge_model: str = "openai/gpt-5-mini",
    judge_api_key: str = None,
    judge_api_base: str = "https://api.vsegpt.ru/v1",
    embedding_model: str = "emb-openai/text-embedding-3-large",
    top_k: int = 5,
    metrics: List[str] = None
) -> Dict[str, Any]:
    """
    –ü–æ–ª–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ RAG —Å–∏—Å—Ç–µ–º—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAGAS 0.4+.
    
    Args:
        pipeline: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π RAG –ø–∞–π–ø–ª–∞–π–Ω
        test_cases: –¢–µ—Å—Ç–æ–≤—ã–µ –∫–µ–π—Å—ã
        judge_model: –ú–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ (–±–µ–∑ –ø—Ä–µ—Ñ–∏–∫—Å–∞ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞)
        judge_api_key: API –∫–ª—é—á –¥–ª—è Judge LLM
        judge_api_base: Base URL –¥–ª—è Judge LLM API
        embedding_model: –ú–æ–¥–µ–ª—å embeddings
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤
        metrics: –°–ø–∏—Å–æ–∫ –º–µ—Ç—Ä–∏–∫ (None = –±–∞–∑–æ–≤—ã–µ)
    
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏
    """
    if not RAGAS_AVAILABLE:
        raise ImportError("RAGAS –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install ragas --upgrade")
    
    # 1. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç (–ø–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç—ã –æ—Ç RAG)
    print("=" * 80)
    print("üìä –®–ê–ì 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞")
    print("=" * 80)
    queries, answers, contexts_list, ground_truths = prepare_rag_evaluation_dataset(
        pipeline, test_cases, top_k
    )
    
    # 2. –°–æ–∑–¥–∞–µ–º –æ—Ü–µ–Ω—â–∏–∫–∞ (Judge LLM)
    print("\n" + "=" * 80)
    print("ü§ñ –®–ê–ì 2: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Judge LLM –¥–ª—è –æ—Ü–µ–Ω–∫–∏")
    print("=" * 80)
    
    # –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω API –∫–ª—é—á, –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–∑ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    if judge_api_key is None:
        judge_api_key = os.getenv("OPENAI_API_KEY")
        if not judge_api_key:
            raise ValueError("–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å judge_api_key –∏–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å OPENAI_API_KEY")
    
    evaluation_client = UniversalLLMClient(
        model=judge_model,
        api_key=judge_api_key,
        api_base=judge_api_base,
        temperature=0.0,  # –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
        max_tokens=32000
    )
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    if metrics is None:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ ground truth
        has_ground_truth = any(gt and gt.strip() for gt in ground_truths)
        
        if has_ground_truth:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            metrics = [
                "faithfulness",
                "answer_relevancy",
                "context_precision",
                "context_recall",
                "context_relevance",
                "response_groundedness",
                "answer_accuracy"
            ]
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–±–µ–∑ ground truth)
            metrics = [
                "faithfulness",
                "context_relevance",
                "response_groundedness"
            ]
    
    evaluator = UniversalRAGEvaluator(
        judge_llm_client=evaluation_client,
        embedding_model=embedding_model,
        metrics=metrics
    )
    
    # 3. –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Ü–µ–Ω–∫—É
    print("\n" + "=" * 80)
    print("üöÄ –®–ê–ì 3: –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ RAGAS")
    print("=" * 80)
    print(f"‚öôÔ∏è  –ú–µ—Ç—Ä–∏–∫–∏: {', '.join(metrics)}")
    print(f"‚è∞ –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç (LLM –≤—ã–∑–æ–≤—ã –¥–ª—è –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–∏)...\n")
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º –æ—Ü–µ–Ω–∫—É
    results = evaluator.evaluate(
        queries=queries,
        answers=answers,
        contexts=contexts_list,
        ground_truths=ground_truths,
        show_progress=True
    )
    
    # 4. –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\n" + "=" * 80)
    print("üìä –®–ê–ì 4: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏")
    print("=" * 80)
    
    evaluator.display_results(max_examples=len(test_cases))
    
    # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\n" + "=" * 80)
    print("üíæ –®–ê–ì 5: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    print("=" * 80)
    
    results_path = evaluator.save_results()
    print(f"‚úÖ –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_path}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∞–∫–∂–µ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    metrics_path = Path("results") / f"ragas_metrics_{timestamp}.json"
    metrics_path.parent.mkdir(parents=True, exist_ok=True)
    with open(metrics_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"‚úÖ –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metrics_path}")
    
    return {
        "aggregated_metrics": results,
        "detailed_results": evaluator.get_detailed_results(),
        "evaluator": evaluator
    }
```


```python
test_cases = [
    {
        "query": "Tell me all about I101 and its relations in Attempto Controlled English",
        "ground_truth": """Okay, let's break down the information about i101 and its relationships as expressed in Attempto Controlled English (ACE) using the provided axioms.

Here's a description, presented in a structured way:

1. What is i101?
According to the axioms, i101 is a length_measure_with_unit. This means i101 represents a measurement involving a length and a unit.

2. Components of i101:
i101 has two key components:
- Value Component: i101_value_component is a length_measure
- Unit Component: i17 is the unit associated with i101

3. i101's Role in Unit Conversion:
i101 serves as a conversion factor for conversion_based_unit i103."""
    },
    {
        "query": "What is the relationship between i101 and i103?",
        "ground_truth": "i101 serves as a conversion factor for i103, where i103 is a conversion_based_unit."
    },
    {
        "query": "What type of measurement is i101?",
        "ground_truth": "i101 is a length_measure_with_unit, which means it represents a length measurement with an associated unit."
    },
    {
        "query": "What is i101_value_component?",
        "ground_truth": "i101_value_component is a length_measure that represents the numerical value component of the i101 measurement."
    },
    {
        "query": "What unit is associated with i101?",
        "ground_truth": "The unit i17 is associated with i101 through the measure_with_unit_has_unit_component relationship."
    }
]
```


```python
evaluation_results = evaluate_rag_with_ragas(
    pipeline=pipeline,
    test_cases=test_cases,
    judge_model="openai/gpt-5-mini",
    judge_api_key="sk-or-vv-xxx",
    judge_api_base="https://api.vsegpt.ru/v1",
    embedding_model="emb-openai/text-embedding-3-large",
    top_k=5,
    metrics=None  # None = –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ ground truth
)
```


```python
# –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–æ—Å—Ç—É–ø–Ω—ã –≤:
# - evaluation_results["aggregated_metrics"] - –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
# - evaluation_results["detailed_results"] - –¥–µ—Ç–∞–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
# - evaluation_results["evaluator"] - –æ–±—ä–µ–∫—Ç evaluator –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
```


```python
for metric, value in evaluation_results["aggregated_metrics"].items():
    print(f"  ‚Ä¢ {metric}: {value:.4f}")
```

      ‚Ä¢ faithfulness_mean: 1.0000
      ‚Ä¢ faithfulness_std: 0.0000
      ‚Ä¢ faithfulness_min: 1.0000
      ‚Ä¢ faithfulness_max: 1.0000
      ‚Ä¢ faithfulness_count: 5.0000
      ‚Ä¢ answer_relevancy_mean: 0.4306
      ‚Ä¢ answer_relevancy_std: 0.3782
      ‚Ä¢ answer_relevancy_min: 0.0000
      ‚Ä¢ answer_relevancy_max: 0.9426
      ‚Ä¢ answer_relevancy_count: 5.0000
      ‚Ä¢ context_precision_mean: 0.1000
      ‚Ä¢ context_precision_std: 0.1225
      ‚Ä¢ context_precision_min: 0.0000
      ‚Ä¢ context_precision_max: 0.2500
      ‚Ä¢ context_precision_count: 5.0000
      ‚Ä¢ context_recall_mean: 0.2500
      ‚Ä¢ context_recall_std: 0.2500
      ‚Ä¢ context_recall_min: 0.0000
      ‚Ä¢ context_recall_max: 0.5000
      ‚Ä¢ context_recall_count: 4.0000
      ‚Ä¢ context_relevance_mean: 0.6000
      ‚Ä¢ context_relevance_std: 0.4062
      ‚Ä¢ context_relevance_min: 0.0000
      ‚Ä¢ context_relevance_max: 1.0000
      ‚Ä¢ context_relevance_count: 5.0000
      ‚Ä¢ response_groundedness_mean: 1.0000
      ‚Ä¢ response_groundedness_std: 0.0000
      ‚Ä¢ response_groundedness_min: 1.0000
      ‚Ä¢ response_groundedness_max: 1.0000
      ‚Ä¢ response_groundedness_count: 5.0000
      ‚Ä¢ answer_accuracy_mean: 0.4000
      ‚Ä¢ answer_accuracy_std: 0.3391
      ‚Ä¢ answer_accuracy_min: 0.0000
      ‚Ä¢ answer_accuracy_max: 0.7500
      ‚Ä¢ answer_accuracy_count: 5.0000
      ‚Ä¢ overall_mean: 0.5401
    


```python
evaluation_results["aggregated_metrics"]
```




    {'faithfulness_mean': 1.0,
     'faithfulness_std': 0.0,
     'faithfulness_min': 1.0,
     'faithfulness_max': 1.0,
     'faithfulness_count': 5,
     'answer_relevancy_mean': 0.43056041541699397,
     'answer_relevancy_std': 0.3781564037159313,
     'answer_relevancy_min': 0.0,
     'answer_relevancy_max': 0.9426166505197333,
     'answer_relevancy_count': 5,
     'context_precision_mean': 0.09999999999,
     'context_precision_std': 0.12247448712691146,
     'context_precision_min': 0.0,
     'context_precision_max': 0.249999999975,
     'context_precision_count': 5,
     'context_recall_mean': 0.25,
     'context_recall_std': 0.25,
     'context_recall_min': 0.0,
     'context_recall_max': 0.5,
     'context_recall_count': 4,
     'context_relevance_mean': 0.6,
     'context_relevance_std': 0.406201920231798,
     'context_relevance_min': 0.0,
     'context_relevance_max': 1.0,
     'context_relevance_count': 5,
     'response_groundedness_mean': 1.0,
     'response_groundedness_std': 0.0,
     'response_groundedness_min': 1.0,
     'response_groundedness_max': 1.0,
     'response_groundedness_count': 5,
     'answer_accuracy_mean': 0.4,
     'answer_accuracy_std': 0.33911649915626346,
     'answer_accuracy_min': 0.0,
     'answer_accuracy_max': 0.75,
     'answer_accuracy_count': 5,
     'overall_mean': 0.5400800593438563}



## –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –æ—Ü–µ–Ω–∫—É


```python
def prepare_rag_evaluation_dataset(
    pipeline,  
    test_cases: List[Dict[str, str]],
    top_k: int = 5,
    save_path: Optional[str] = None
) -> pd.DataFrame:
    """
    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ RAGAS.
    
    Args:
        pipeline: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π RAG –ø–∞–π–ø–ª–∞–π–Ω
        test_cases: –°–ø–∏—Å–æ–∫ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∫–µ–π—Å–æ–≤ —Å query –∏ ground_truth
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        save_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ (optional)
    
    Returns:
        DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
    """
    data = []
    
    print(f"üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è {len(test_cases)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤...")
    
    for idx, test_case in enumerate(tqdm(test_cases, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤")):
        query = test_case["query"]
        ground_truth = test_case.get("ground_truth", "")
        metadata = test_case.get("metadata", {})
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã –æ—Ç RAG —Å–∏—Å—Ç–µ–º—ã
            result = pipeline.query(
                query=query,
                top_k=top_k,
                return_details=True,
                display_result=False
            )
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø–∏—Å—å
            row = {
                'idx': idx,
                'query': query,
                'answer': result["answer"],
                'contexts': '|||'.join(result["contexts"]),  # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã
                'ground_truth': ground_truth,
                'num_contexts': len(result["contexts"]),
                'timestamp': datetime.now().isoformat(),
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            row.update(metadata)
            
            data.append(row)
            
            print(f"‚úÖ [{idx+1}/{len(test_cases)}] –û–±—Ä–∞–±–æ—Ç–∞–Ω: {query[:50]}...")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞ '{query[:50]}...': {e}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—É—é –∑–∞–ø–∏—Å—å
            data.append({
                'idx': idx,
                'query': query,
                'answer': "",
                'contexts': "",
                'ground_truth': ground_truth,
                'num_contexts': 0,
                'error': str(e),
                'timestamp': datetime.now().isoformat(),
            })
    
    # –°–æ–∑–¥–∞–µ–º DataFrame
    df = pd.DataFrame(data)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –ø—É—Ç—å
    if save_path:
        save_path = Path(save_path)
        save_path.parent.mkdir(parents=True, exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Parquet (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö)
        if save_path.suffix == '.parquet':
            df.to_parquet(save_path, index=False, compression='gzip')
        # –ò–ª–∏ –≤ CSV
        elif save_path.suffix == '.csv':
            df.to_csv(save_path, index=False, encoding='utf-8')
        # JSON
        elif save_path.suffix == '.json':
            df.to_json(save_path, orient='records', lines=True, force_ascii=False)
        else:
            # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é Parquet
            save_path = save_path.with_suffix('.parquet')
            df.to_parquet(save_path, index=False, compression='gzip')
        
        print(f"\nüíæ –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {save_path}")
        print(f"   –§–æ—Ä–º–∞—Ç: {save_path.suffix}")
        print(f"   –†–∞–∑–º–µ—Ä: {save_path.stat().st_size / 1024:.1f} KB")
    
    print(f"\n‚úÖ –î–∞—Ç–∞—Å–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω:")
    print(f"   ‚Ä¢ –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}")
    print(f"   ‚Ä¢ –£—Å–ø–µ—à–Ω—ã—Ö: {df['answer'].notna().sum()}")
    print(f"   ‚Ä¢ –° ground truth: {df['ground_truth'].notna().sum()}")
    print(f"   ‚Ä¢ –° –æ—à–∏–±–∫–∞–º–∏: {df.get('error', pd.Series()).notna().sum()}")
    
    return df
```


```python
def load_evaluation_dataset(filepath: str) -> pd.DataFrame:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏–∑ —Ñ–∞–π–ª–∞.
    
    Args:
        filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–∞—Ç–∞—Å–µ—Ç–∞
    
    Returns:
        DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
    """
    filepath = Path(filepath)
    
    if not filepath.exists():
        raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {filepath}")
    
    print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {filepath}")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º
    if filepath.suffix == '.parquet':
        df = pd.read_parquet(filepath)
    elif filepath.suffix == '.csv':
        df = pd.read_csv(filepath, encoding='utf-8')
    elif filepath.suffix == '.json':
        df = pd.read_json(filepath, orient='records', lines=True)
    else:
        raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {filepath.suffix}")
    
    print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω:")
    print(f"   ‚Ä¢ –ó–∞–ø–∏—Å–µ–π: {len(df)}")
    print(f"   ‚Ä¢ –ö–æ–ª–æ–Ω–æ–∫: {len(df.columns)}")
    print(f"   ‚Ä¢ –†–∞–∑–º–µ—Ä: {filepath.stat().st_size / 1024:.1f} KB")
    
    return df
```


```python
def dataset_to_ragas_format(df: pd.DataFrame) -> Tuple[List[str], List[str], List[List[str]], List[str]]:
    """
    –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç DataFrame –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è RAGAS –æ—Ü–µ–Ω–∫–∏.
    
    Args:
        df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
    
    Returns:
        –ö–æ—Ä—Ç–µ–∂ (queries, answers, contexts_list, ground_truths)
    """
    queries = df['query'].tolist()
    answers = df['answer'].tolist()
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã
    contexts_list = []
    for contexts_str in df['contexts']:
        if pd.notna(contexts_str) and contexts_str:
            contexts = contexts_str.split('|||')
            contexts_list.append(contexts)
        else:
            contexts_list.append([])
    
    ground_truths = df['ground_truth'].fillna("").tolist()
    
    return queries, answers, contexts_list, ground_truths
```


```python
def evaluate_rag_with_ragas(
    dataset: Optional[Union[pd.DataFrame, str]] = None,
    pipeline = None,
    test_cases: List[Dict[str, str]] = None,
    judge_model: str = "openai/gpt-5-mini",
    judge_api_key: str = None,
    judge_api_base: str = "https://api.vsegpt.ru/v1",
    embedding_model: str = "emb-openai/text-embedding-3-large",
    top_k: int = 5,
    metrics: List[str] = None,
    max_tokens: int = 32000,  # ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä
    save_dataset_path: Optional[str] = None,
    enable_timing: bool = True,  # ‚úÖ –ù–æ–≤—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä
    exclude_slow_metrics: bool = False,  # ‚úÖ –ù–æ–≤—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä
    max_metric_time: float = 15.0  # ‚úÖ –ù–æ–≤—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä
) -> Dict[str, Any]:
    """
    –ü–æ–ª–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ RAG —Å–∏—Å—Ç–µ–º—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAGAS 0.4+.
    
    Args:
        dataset: DataFrame –∏–ª–∏ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –≥–æ—Ç–æ–≤—ã–º –¥–∞—Ç–∞—Å–µ—Ç–æ–º
        pipeline: RAG –ø–∞–π–ø–ª–∞–π–Ω (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç)
        test_cases: –¢–µ—Å—Ç–æ–≤—ã–µ –∫–µ–π—Å—ã (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç)
        judge_model: –ú–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        judge_api_key: API –∫–ª—é—á –¥–ª—è Judge LLM
        judge_api_base: Base URL –¥–ª—è Judge LLM API
        embedding_model: –ú–æ–¥–µ–ª—å embeddings
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤
        metrics: –°–ø–∏—Å–æ–∫ –º–µ—Ç—Ä–∏–∫ (None = –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
        max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è LLM
        save_dataset_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        enable_timing: –í–∫–ª—é—á–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        exclude_slow_metrics: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–∫–ª—é—á–∏—Ç—å –º–µ–¥–ª–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        max_metric_time: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –¥–ª—è –º–µ—Ç—Ä–∏–∫–∏ (–µ—Å–ª–∏ exclude_slow_metrics=True)
    
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏
    """
    if not RAGAS_AVAILABLE:
        raise ImportError("RAGAS –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install ragas --upgrade")
    
    # 1. –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    print("=" * 80)
    print("üìä –®–ê–ì 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞")
    print("=" * 80)
    
    if dataset is not None:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç
        if isinstance(dataset, str):
            df = load_evaluation_dataset(dataset)
        else:
            df = dataset
            print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π DataFrame ({len(df)} –∑–∞–ø–∏—Å–µ–π)")
    else:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        if pipeline is None or test_cases is None:
            raise ValueError("–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å –ª–∏–±–æ dataset, –ª–∏–±–æ pipeline+test_cases")
        
        df = prepare_rag_evaluation_dataset(
            pipeline=pipeline,
            test_cases=test_cases,
            top_k=top_k,
            save_path=save_dataset_path
        )
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç RAGAS
    queries, answers, contexts_list, ground_truths = dataset_to_ragas_format(df)
    
    # 2. –°–æ–∑–¥–∞–µ–º –æ—Ü–µ–Ω—â–∏–∫–∞ (Judge LLM)
    print("\n" + "=" * 80)
    print("ü§ñ –®–ê–ì 2: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Judge LLM –¥–ª—è –æ—Ü–µ–Ω–∫–∏")
    print("=" * 80)
    
    if judge_api_key is None:
        judge_api_key = os.getenv("OPENAI_API_KEY")
        if not judge_api_key:
            raise ValueError("–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å judge_api_key –∏–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å OPENAI_API_KEY")
    
    # ‚úÖ –ü–µ—Ä–µ–¥–∞–µ–º max_tokens
    evaluation_client = UniversalLLMClient(
        model=judge_model,
        api_key=judge_api_key,
        api_base=judge_api_base,
        temperature=0.0,
        max_tokens=max_tokens  # ‚úÖ –¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
    )
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    if metrics is None:
        has_ground_truth = any(gt and gt.strip() for gt in ground_truths)
        
        if has_ground_truth:
            metrics = [
                "faithfulness",
                "answer_relevancy",
                "context_precision",
                "context_recall",
                "context_relevance",
                "response_groundedness",
                "answer_accuracy"
            ]
        else:
            metrics = [
                "faithfulness",
                "context_relevance",
                "response_groundedness"
            ]
    
    # ‚úÖ –°–æ–∑–¥–∞–µ–º evaluator —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π timing
    evaluator = UniversalRAGEvaluator(
        judge_llm_client=evaluation_client,
        embedding_model=embedding_model,
        metrics=metrics,
        enable_timing=enable_timing  # ‚úÖ –í–∫–ª—é—á–∞–µ–º –∞–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–∏
    )
    
    # 3. –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Ü–µ–Ω–∫—É
    print("\n" + "=" * 80)
    print("üöÄ –®–ê–ì 3: –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ RAGAS")
    print("=" * 80)
    print(f"‚öôÔ∏è  –ú–µ—Ç—Ä–∏–∫–∏: {', '.join(metrics)}")
    print(f"‚öôÔ∏è  max_tokens: {max_tokens}")
    print(f"‚è∞ –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç (LLM –≤—ã–∑–æ–≤—ã –¥–ª—è –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–∏)...\n")
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º –æ—Ü–µ–Ω–∫—É
    results = evaluator.evaluate(
        queries=queries,
        answers=answers,
        contexts=contexts_list,
        ground_truths=ground_truths,
        show_progress=True
    )
    
    # 4. –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    if enable_timing:
        print("\n" + "=" * 80)
        print("‚è±Ô∏è –®–ê–ì 3.5: –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ—Ç—Ä–∏–∫")
        print("=" * 80)
        
        evaluator.display_timing_analysis()
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        if exclude_slow_metrics:
            recommended_metrics = evaluator.get_recommended_metrics(
                max_avg_time=max_metric_time,
                exclude_redundant=True
            )
            
            print(f"\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ—Ç—Ä–∏–∫–∏ {recommended_metrics}")
            print(f"   –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ—Ü–µ–Ω–∫–∏ –≤ {len(metrics)/len(recommended_metrics):.1f}x —Ä–∞–∑")
    
    # 5. –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\n" + "=" * 80)
    print("üìä –®–ê–ì 4: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏")
    print("=" * 80)
    
    evaluator.display_results(max_examples=len(queries))
    
    # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\n" + "=" * 80)
    print("üíæ –®–ê–ì 5: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    print("=" * 80)
    
    results_path = evaluator.save_results()
    print(f"‚úÖ –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_path}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    metrics_path = Path("results") / f"ragas_metrics_{timestamp}.json"
    metrics_path.parent.mkdir(parents=True, exist_ok=True)
    with open(metrics_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"‚úÖ –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metrics_path}")
    
    # ‚úÖ –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    if enable_timing:
        timing_df = evaluator.get_timing_statistics()
        timing_path = Path("results") / f"ragas_timing_{timestamp}.csv"
        timing_df.to_csv(timing_path, index=False)
        print(f"‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {timing_path}")
    
    return {
        "aggregated_metrics": results,
        "detailed_results": evaluator.get_detailed_results(),
        "evaluator": evaluator,
        "dataset": df,
        "timing_statistics": evaluator.get_timing_statistics() if enable_timing else None
    }
```

### –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º


```python
dataset_df = prepare_rag_evaluation_dataset(
    pipeline=pipeline,
    test_cases=test_cases,
    top_k=5,
    save_path="datasets/rag_evaluation_dataset.parquet"
)
```

### –û—Ü–µ–Ω–∫–∞ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –¥–∞—Ç–∞—Å–µ—Ç–æ–º


```python
results = evaluate_rag_with_ragas(
    dataset="datasets/rag_evaluation_dataset.parquet",  # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
    judge_model="openai/gpt-5-mini",
    judge_api_key="sk-or-vv-xxx",
    judge_api_base="https://api.vsegpt.ru/v1",
    embedding_model="emb-openai/text-embedding-3-large",
    max_tokens=32000,
    enable_timing=True,  # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    exclude_slow_metrics=False,  # –ü–æ–∫–∞ –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏
    metrics=None  # –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
)
```


```python
recommended = results['evaluator'].get_recommended_metrics(
    max_avg_time=10.0,
    exclude_redundant=True
)
```

    2026-01-09 19:49:33,506 - __main__ - INFO - ‚úÖ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–µ—Ç—Ä–∏–∫–∏: 
    

### –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏


```python
metrics = [
    # "faithfulness",
    "answer_relevancy",
    # "context_precision",
    "context_recall",
    "context_relevance",
    # "response_groundedness",
    "answer_accuracy"
]
```


```python
start_time = time.time()
results_fast = evaluate_rag_with_ragas(
    dataset="datasets/rag_evaluation_dataset.parquet",
    judge_model="openai/gpt-5-mini",
    judge_api_key="sk-or-vv-xxx",
    max_tokens=32000,
    metrics=metrics, #recommended,  # –¢–æ–ª—å–∫–æ –±—ã—Å—Ç—Ä—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    enable_timing=True
)
elapsed_time = time.time() - start_time
print(f"–í—Ä–µ–º—è –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è: {elapsed_time} —Å–µ–∫.")
```

## –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞


```python
def load_test_cases_from_file(filepath: str) -> List[Dict[str, str]]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–µ –∫–µ–π—Å—ã –∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞.
    
    Args:
        filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É (CSV, Parquet –∏–ª–∏ JSON)
    
    Returns:
        –°–ø–∏—Å–æ–∫ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∫–µ–π—Å–æ–≤
    """
    filepath = Path(filepath)
    
    if not filepath.exists():
        raise FileNotFoundError(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {filepath}")
    
    print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∫–µ–π—Å–æ–≤: {filepath}")
    
    # JSON - –ø—Ä—è–º–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
    if filepath.suffix == '.json':
        with open(filepath, 'r', encoding='utf-8') as f:
            test_cases = json.load(f)
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(test_cases)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –∫–µ–π—Å–æ–≤ –∏–∑ JSON")
        return test_cases
    
    # CSV –∏–ª–∏ Parquet - –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏–∑ DataFrame
    if filepath.suffix == '.csv':
        df = pd.read_csv(filepath, encoding='utf-8')
    elif filepath.suffix == '.parquet':
        df = pd.read_parquet(filepath)
    else:
        raise ValueError(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {filepath.suffix}")
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
    test_cases = []
    for _, row in df.iterrows():
        test_case = {
            "query": row["query"],
            "ground_truth": row["ground_truth"],
            "metadata": {
                "object_id": row.get("object_id", ""),
                "num_axioms": int(row.get("num_axioms", 0)),
            }
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∞–∫—Å–∏–æ–º—ã –µ—Å–ª–∏ –µ—Å—Ç—å
        if "axioms" in row and pd.notna(row["axioms"]):
            axioms_str = str(row["axioms"])
            test_case["metadata"]["axioms"] = axioms_str.split("\n") if axioms_str else []
        
        test_cases.append(test_case)
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(test_cases)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –∫–µ–π—Å–æ–≤")
    return test_cases
```


```python
loaded_test_cases = load_test_cases_from_file("./datasets/rag_test_cases_20260109_222314.json")

print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(loaded_test_cases)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –∫–µ–π—Å–æ–≤")
```

    üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∫–µ–π—Å–æ–≤: datasets\rag_test_cases_20260109_222314.json
    ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ 198 —Ç–µ—Å—Ç–æ–≤—ã—Ö –∫–µ–π—Å–æ–≤ –∏–∑ JSON
    ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ 198 —Ç–µ—Å—Ç–æ–≤—ã—Ö –∫–µ–π—Å–æ–≤
    

### –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ RAGAS


```python
# –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–µ test_cases –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ –æ—Ç RAG —Å–∏—Å—Ç–µ–º—ã
dataset_df = prepare_rag_evaluation_dataset(
    pipeline=pipeline,  # –í–∞—à RAG pipeline
    test_cases=loaded_test_cases,
    top_k=5,
    save_path="./datasets/rag_evaluation_dataset_full.parquet"
)
```

### –û—Ü–µ–Ω–∫–∞ RAGAS


```python
metrics = [
    # "faithfulness",
    "answer_relevancy",
    # "context_precision",
    "context_recall",
    "context_relevance",
    # "response_groundedness",
    "answer_accuracy"
]
```


```python
evaluation_results = evaluate_rag_with_ragas(
    dataset=dataset_df,
    judge_model="openai/gpt-5-mini",
    judge_api_key="sk-or-vv-xxx",
    judge_api_base="https://api.vsegpt.ru/v1",
    max_tokens=32000,
    enable_timing=True,
    metrics=metrics
)
```

# RAGAS –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è GraphRAG

### –ê–¥–∞–ø—Ç–µ—Ä GraphRAG –¥–ª—è RAGAS


```python
class GraphRAGAdapter:
    """
    –ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ Microsoft GraphRAG —Å RAGAS.
    
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ GraphRAG (—Å—É—â–Ω–æ—Å—Ç–∏, —Å–≤—è–∑–∏)
    –≤ —Ñ–æ—Ä–º–∞—Ç, —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ RAGAS.
    """
    
    def __init__(self, search_engine):
        """
        Args:
            search_engine: –≠–∫–∑–µ–º–ø–ª—è—Ä LocalSearch –∏–∑ GraphRAG
        """
        self.search_engine = search_engine
        self.logger = logging.getLogger(__name__)
    
    async def query(self, 
                   question: str,
                   return_details: bool = True) -> Dict[str, Any]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ GraphRAG –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è RAGAS.
        
        Args:
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            return_details: –í–æ–∑–≤—Ä–∞—â–∞—Ç—å –ª–∏ –¥–µ—Ç–∞–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –æ—Ç–≤–µ—Ç–æ–º –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        """
        start_time = time.time()
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ –≤ GraphRAG
        result = await self.search_engine.search(question)
        
        elapsed_time = time.time() - start_time
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        answer = result.response
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        contexts = []
        context_data = {}
        
        if return_details and hasattr(result, 'context_data'):
            contexts, context_data = self._extract_contexts(result.context_data)
        
        return {
            "answer": answer,
            "contexts": contexts,
            "context_data": context_data,
            "elapsed_time": elapsed_time,
            "raw_result": result
        }
    
    def _extract_contexts(self, context_data: Dict) -> Tuple[List[str], Dict]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ GraphRAG context_data.
        
        GraphRAG –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç:
        - entities: DataFrame —Å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º–∏ —Å—É—â–Ω–æ—Å—Ç—è–º–∏
        - relationships: DataFrame —Å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º–∏ —Å–≤—è–∑—è–º–∏
        - reports: –û—Ç—á–µ—Ç—ã —Å–æ–æ–±—â–µ—Å—Ç–≤
        - sources: –¢–µ–∫—Å—Ç–æ–≤—ã–µ –µ–¥–∏–Ω–∏—Ü—ã
        
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏—Ö –≤ —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ (contexts) –¥–ª—è RAGAS.
        
        Args:
            context_data: –°–ª–æ–≤–∞—Ä—å —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –æ—Ç GraphRAG
        
        Returns:
            –ö–æ—Ä—Ç–µ–∂ (—Å–ø–∏—Å–æ–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö —Å—Ç—Ä–æ–∫, —Å–ª–æ–≤–∞—Ä—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö)
        """
        contexts = []
        metadata = {
            "num_entities": 0,
            "num_relationships": 0,
            "num_reports": 0,
            "num_sources": 0
        }
        
        # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏
        if "entities" in context_data and context_data["entities"] is not None:
            entities_df = context_data["entities"]
            
            if not entities_df.empty:
                metadata["num_entities"] = len(entities_df)
                
                # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é —Å—É—â–Ω–æ—Å—Ç—å
                for _, entity in entities_df.iterrows():
                    entity_context = self._format_entity(entity)
                    if entity_context:
                        contexts.append(entity_context)
        
        # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–≤—è–∑–∏
        if "relationships" in context_data and context_data["relationships"] is not None:
            relationships_df = context_data["relationships"]
            
            if not relationships_df.empty:
                metadata["num_relationships"] = len(relationships_df)
                
                # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å–≤—è–∑–∏
                for _, rel in relationships_df.iterrows():
                    rel_context = self._format_relationship(rel)
                    if rel_context:
                        contexts.append(rel_context)
        
        # 3. –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç—á–µ—Ç—ã —Å–æ–æ–±—â–µ—Å—Ç–≤
        if "reports" in context_data and context_data["reports"] is not None:
            reports_df = context_data["reports"]
            
            if not reports_df.empty:
                metadata["num_reports"] = len(reports_df)
                
                for _, report in reports_df.iterrows():
                    report_context = self._format_report(report)
                    if report_context:
                        contexts.append(report_context)
        
        # 4. –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –µ–¥–∏–Ω–∏—Ü—ã (sources)
        if "sources" in context_data and context_data["sources"] is not None:
            sources_df = context_data["sources"]
            
            if not sources_df.empty:
                metadata["num_sources"] = len(sources_df)
                
                for _, source in sources_df.iterrows():
                    source_context = self._format_source(source)
                    if source_context:
                        contexts.append(source_context)
        
        self.logger.info(
            f"üìä –ò–∑–≤–ª–µ—á–µ–Ω–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤: {len(contexts)} "
            f"(entities: {metadata['num_entities']}, "
            f"relationships: {metadata['num_relationships']}, "
            f"reports: {metadata['num_reports']}, "
            f"sources: {metadata['num_sources']})"
        )
        
        return contexts, metadata
    
    def _format_entity(self, entity: pd.Series) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å—É—â–Ω–æ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–æ–≤—É—é —Å—Ç—Ä–æ–∫—É.
        
        –¢–∏–ø–∏—á–Ω—ã–µ –ø–æ–ª—è –≤ GraphRAG entity:
        - title/name: –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏
        - type: —Ç–∏–ø —Å—É—â–Ω–æ—Å—Ç–∏
        - description: –æ–ø–∏—Å–∞–Ω–∏–µ
        - rank: –≤–∞–∂–Ω–æ—Å—Ç—å
        """
        parts = []
        
        # –ù–∞–∑–≤–∞–Ω–∏–µ
        title = entity.get('title') or entity.get('name', 'Unknown Entity')
        parts.append(f"Entity: {title}")
        
        # –¢–∏–ø
        if 'type' in entity and pd.notna(entity['type']):
            parts.append(f"Type: {entity['type']}")
        
        # –û–ø–∏—Å–∞–Ω–∏–µ
        if 'description' in entity and pd.notna(entity['description']):
            desc = str(entity['description']).strip()
            if desc:
                parts.append(f"Description: {desc}")
        
        # –†–∞–Ω–≥/–≤–∞–∂–Ω–æ—Å—Ç—å
        if 'rank' in entity and pd.notna(entity['rank']):
            parts.append(f"Rank: {entity['rank']}")
        
        return " | ".join(parts) if parts else ""
    
    def _format_relationship(self, relationship: pd.Series) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–≤—è–∑—å –≤ —Ç–µ–∫—Å—Ç–æ–≤—É—é —Å—Ç—Ä–æ–∫—É.
        
        –¢–∏–ø–∏—á–Ω—ã–µ –ø–æ–ª—è:
        - source: –∏—Å—Ö–æ–¥–Ω–∞—è —Å—É—â–Ω–æ—Å—Ç—å
        - target: —Ü–µ–ª–µ–≤–∞—è —Å—É—â–Ω–æ—Å—Ç—å
        - description: –æ–ø–∏—Å–∞–Ω–∏–µ —Å–≤—è–∑–∏
        - weight: –≤–µ—Å —Å–≤—è–∑–∏
        """
        parts = []
        
        source = relationship.get('source', 'Unknown')
        target = relationship.get('target', 'Unknown')
        
        parts.append(f"Relationship: {source} -> {target}")
        
        # –û–ø–∏—Å–∞–Ω–∏–µ —Å–≤—è–∑–∏
        if 'description' in relationship and pd.notna(relationship['description']):
            desc = str(relationship['description']).strip()
            if desc:
                parts.append(f"Description: {desc}")
        
        # –í–µ—Å
        if 'weight' in relationship and pd.notna(relationship['weight']):
            parts.append(f"Weight: {relationship['weight']}")
        
        return " | ".join(parts) if parts else ""
    
    def _format_report(self, report: pd.Series) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç —Å–æ–æ–±—â–µ—Å—Ç–≤–∞.
        
        –¢–∏–ø–∏—á–Ω—ã–µ –ø–æ–ª—è:
        - title: –Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        - summary: –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
        - full_content: –ø–æ–ª–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
        - rank: –≤–∞–∂–Ω–æ—Å—Ç—å
        """
        parts = []
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        if 'title' in report and pd.notna(report['title']):
            parts.append(f"Community Report: {report['title']}")
        
        # –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ (–∏—Å–ø–æ–ª—å–∑—É–µ–º summary –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ full_content)
        content = None
        if 'summary' in report and pd.notna(report['summary']):
            content = str(report['summary']).strip()
        elif 'full_content' in report and pd.notna(report['full_content']):
            content = str(report['full_content']).strip()
        
        if content:
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –¥–ª—è —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏
            if len(content) > 500:
                content = content[:497] + "..."
            parts.append(f"Content: {content}")
        
        return " | ".join(parts) if parts else ""
    
    def _format_source(self, source: pd.Series) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—É—é –µ–¥–∏–Ω–∏—Ü—É (source).
        
        –¢–∏–ø–∏—á–Ω—ã–µ –ø–æ–ª—è:
        - text: —Ç–µ–∫—Å—Ç
        - id: –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        """
        if 'text' in source and pd.notna(source['text']):
            text = str(source['text']).strip()
            
            # –î–æ–±–∞–≤–ª—è–µ–º ID –µ—Å–ª–∏ –µ—Å—Ç—å
            if 'id' in source and pd.notna(source['id']):
                return f"Source [{source['id']}]: {text}"
            else:
                return f"Source: {text}"
        
        return ""
```


```python
def sync_query_graphrag(adapter: GraphRAGAdapter, question: str) -> Dict[str, Any]:
    """
    –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ query GraphRAG.
    
    Args:
        adapter: GraphRAGAdapter —ç–∫–∑–µ–º–ø–ª—è—Ä
        question: –í–æ–ø—Ä–æ—Å
    
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞–ø—Ä–æ—Å–∞
    """
    try:
        # –ü—Ä–∏–º–µ–Ω—è–µ–º nest_asyncio –µ—Å–ª–∏ –Ω—É–∂–Ω–æ (–¥–ª—è Jupyter)
        import nest_asyncio
        nest_asyncio.apply()
    except:
        pass
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
    try:
        loop = asyncio.get_running_loop()
        result = loop.run_until_complete(adapter.query(question))
    except RuntimeError:
        result = asyncio.run(adapter.query(question))
    
    return result
```

### –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è GraphRAG


```python
def prepare_graphrag_evaluation_dataset(
    graphrag_adapter: GraphRAGAdapter,
    test_cases: List[Dict[str, str]],
    save_path: Optional[str] = None,
    verbose: bool = True
) -> pd.DataFrame:
    """
    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ GraphRAG —Å RAGAS.
    
    Args:
        graphrag_adapter: –ê–¥–∞–ø—Ç–µ—Ä GraphRAG
        test_cases: –°–ø–∏—Å–æ–∫ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∫–µ–π—Å–æ–≤ —Å query –∏ ground_truth
        save_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
        verbose: –ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥
    
    Returns:
        DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
    """
    data = []
    
    print(f"üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ GraphRAG –¥–ª—è {len(test_cases)} –∑–∞–ø—Ä–æ—Å–æ–≤...")
    
    iterator = tqdm(test_cases, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤") if verbose else test_cases
    
    for idx, test_case in enumerate(iterator):
        query = test_case["query"]
        ground_truth = test_case.get("ground_truth", "")
        metadata = test_case.get("metadata", {})
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç GraphRAG
            result = sync_query_graphrag(graphrag_adapter, query)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø–∏—Å—å
            row = {
                'idx': idx,
                'query': query,
                'answer': result["answer"],
                'contexts': '|||'.join(result["contexts"]),  # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã
                'ground_truth': ground_truth,
                'num_contexts': len(result["contexts"]),
                'num_entities': result["context_data"].get("num_entities", 0),
                'num_relationships': result["context_data"].get("num_relationships", 0),
                'num_reports': result["context_data"].get("num_reports", 0),
                'num_sources': result["context_data"].get("num_sources", 0),
                'elapsed_time': result["elapsed_time"],
                'timestamp': datetime.now().isoformat(),
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            row.update(metadata)
            
            data.append(row)
            
            if verbose:
                tqdm.write(
                    f"‚úÖ [{idx+1}/{len(test_cases)}] {query[:50]}... "
                    f"(–∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤: {row['num_contexts']}, –≤—Ä–µ–º—è: {row['elapsed_time']:.1f}—Å)"
                )
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞ '{query[:50]}...': {e}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—É—é –∑–∞–ø–∏—Å—å
            data.append({
                'idx': idx,
                'query': query,
                'answer': "",
                'contexts': "",
                'ground_truth': ground_truth,
                'num_contexts': 0,
                'num_entities': 0,
                'num_relationships': 0,
                'num_reports': 0,
                'num_sources': 0,
                'elapsed_time': 0,
                'error': str(e),
                'timestamp': datetime.now().isoformat(),
            })
    
    # –°–æ–∑–¥–∞–µ–º DataFrame
    df = pd.DataFrame(data)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –ø—É—Ç—å
    if save_path:
        save_path = Path(save_path)
        save_path.parent.mkdir(parents=True, exist_ok=True)
        
        if save_path.suffix == '.parquet':
            df.to_parquet(save_path, index=False, compression='gzip')
        elif save_path.suffix == '.csv':
            df.to_csv(save_path, index=False, encoding='utf-8')
        else:
            save_path = save_path.with_suffix('.parquet')
            df.to_parquet(save_path, index=False, compression='gzip')
        
        print(f"\nüíæ –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {save_path}")
        print(f"   –†–∞–∑–º–µ—Ä: {save_path.stat().st_size / 1024:.1f} KB")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\n‚úÖ –î–∞—Ç–∞—Å–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω:")
    print(f"   ‚Ä¢ –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}")
    print(f"   ‚Ä¢ –£—Å–ø–µ—à–Ω—ã—Ö: {df['answer'].notna().sum()}")
    print(f"   ‚Ä¢ –° ground truth: {df['ground_truth'].notna().sum()}")
    print(f"   ‚Ä¢ –° –æ—à–∏–±–∫–∞–º–∏: {df.get('error', pd.Series()).notna().sum()}")
    print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤: {df['num_contexts'].mean():.1f}")
    print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {df['elapsed_time'].mean():.1f}—Å")
    
    return df
```

### –ê–Ω–∞–ª–∏–∑ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –º–µ—Ç—Ä–∏–∫ RAGAS —Å GraphRAG


```python
def analyze_graphrag_ragas_compatibility(df: pd.DataFrame) -> Dict[str, Any]:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –º–µ—Ç—Ä–∏–∫ RAGAS —Å –¥–∞–Ω–Ω—ã–º–∏ GraphRAG.
    
    Args:
        df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ GraphRAG
    
    Returns:
        –û—Ç—á–µ—Ç –æ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    """
    from IPython.display import display, HTML
    
    compatibility = {
        "faithfulness": {
            "compatible": True,
            "required_fields": ["answer", "contexts"],
            "notes": "‚úÖ –ü–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–≤–º–µ—Å—Ç–∏–º–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã –∏–∑ —Å—É—â–Ω–æ—Å—Ç–µ–π/—Å–≤—è–∑–µ–π/–æ—Ç—á–µ—Ç–æ–≤."
        },
        "answer_relevancy": {
            "compatible": True,
            "required_fields": ["query", "answer"],
            "notes": "‚úÖ –ü–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–≤–º–µ—Å—Ç–∏–º–∞. –ù–µ —Ç—Ä–µ–±—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç embeddings."
        },
        "context_precision": {
            "compatible": True,
            "required_fields": ["query", "contexts", "ground_truth"],
            "notes": "‚úÖ –°–æ–≤–º–µ—Å—Ç–∏–º–∞ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ ground_truth. –û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤."
        },
        "context_recall": {
            "compatible": True,
            "required_fields": ["contexts", "ground_truth"],
            "notes": "‚úÖ –°–æ–≤–º–µ—Å—Ç–∏–º–∞ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ ground_truth. –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–ª–Ω–æ—Ç—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."
        },
        "context_relevance": {
            "compatible": True,
            "required_fields": ["query", "contexts"],
            "notes": "‚úÖ –ü–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–≤–º–µ—Å—Ç–∏–º–∞. –û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å—É."
        },
        "response_groundedness": {
            "compatible": True,
            "required_fields": ["answer", "contexts"],
            "notes": "‚úÖ –ü–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–≤–º–µ—Å—Ç–∏–º–∞. –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º."
        },
        "answer_accuracy": {
            "compatible": True,
            "required_fields": ["query", "answer", "ground_truth"],
            "notes": "‚úÖ –°–æ–≤–º–µ—Å—Ç–∏–º–∞ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ ground_truth. –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –æ—Ç–≤–µ—Ç —Å —ç—Ç–∞–ª–æ–Ω–æ–º."
        }
    }
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
    has_contexts = df['contexts'].notna().sum() > 0
    has_ground_truth = df['ground_truth'].notna().sum() > 0
    avg_contexts = df['num_contexts'].mean()
    
    # –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    recommended_metrics = []
    
    if has_contexts:
        recommended_metrics.extend([
            "faithfulness",
            "context_relevance",
            "response_groundedness"
        ])
    
    if has_contexts and has_ground_truth:
        recommended_metrics.extend([
            "context_precision",
            "context_recall"
        ])
    
    if has_ground_truth:
        recommended_metrics.append("answer_accuracy")
    
    # Always available
    recommended_metrics.append("answer_relevancy")
    
    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    recommended_metrics = list(dict.fromkeys(recommended_metrics))
    
    # HTML –æ—Ç—á–µ—Ç
    html = f"""
    <div style="border: 2px solid #2196F3; border-radius: 12px; padding: 20px; margin: 15px 0;
                background: linear-gradient(135deg, #E3F2FD 0%, #BBDEFB 100%);">
        <h2 style="color: #1565C0; margin-top: 0;">üîç –ê–Ω–∞–ª–∏–∑ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ RAGAS —Å GraphRAG</h2>
        
        <div style="background: white; padding: 15px; border-radius: 8px; margin: 15px 0;">
            <h3 style="color: #1565C0;">üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö</h3>
            <ul style="line-height: 1.8;">
                <li>üìù –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: <strong>{len(df)}</strong></li>
                <li>üì¶ –ó–∞–ø–∏—Å–µ–π —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º–∏: <strong>{df['contexts'].notna().sum()}</strong> ({df['contexts'].notna().sum()/len(df)*100:.1f}%)</li>
                <li>‚úÖ –ó–∞–ø–∏—Å–µ–π —Å ground truth: <strong>{df['ground_truth'].notna().sum()}</strong> ({df['ground_truth'].notna().sum()/len(df)*100:.1f}%)</li>
                <li>üìä –°—Ä–µ–¥–Ω–µ–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ –∑–∞–ø—Ä–æ—Å: <strong>{avg_contexts:.1f}</strong></li>
                <li>üîó –°—Ä–µ–¥–Ω–µ–µ —Å—É—â–Ω–æ—Å—Ç–µ–π: <strong>{df['num_entities'].mean():.1f}</strong></li>
                <li>üîó –°—Ä–µ–¥–Ω–µ–µ —Å–≤—è–∑–µ–π: <strong>{df['num_relationships'].mean():.1f}</strong></li>
            </ul>
        </div>
        
        <div style="background: white; padding: 15px; border-radius: 8px; margin: 15px 0;">
            <h3 style="color: #1565C0;">‚úÖ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–µ—Ç—Ä–∏–∫–∏ RAGAS</h3>
            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 10px;">
    """
    
    for metric in recommended_metrics:
        info = compatibility.get(metric, {})
        html += f"""
                <div style="background: #E8F5E9; padding: 10px; border-radius: 5px; border-left: 4px solid #4CAF50;">
                    <strong style="color: #2E7D32;">{metric}</strong>
                    <div style="font-size: 0.85em; color: #666; margin-top: 5px;">
                        {info.get('notes', '')}
                    </div>
                </div>
        """
    
    html += """
            </div>
        </div>
        
        <div style="background: #FFF9C4; padding: 15px; border-radius: 8px; margin: 15px 0;">
            <h3 style="color: #F57F17;">‚ö†Ô∏è –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ GraphRAG</h3>
            <ul style="line-height: 1.8; color: #666;">
                <li>GraphRAG –∏—Å–ø–æ–ª—å–∑—É–µ—Ç <strong>—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç</strong> (—Å—É—â–Ω–æ—Å—Ç–∏ + —Å–≤—è–∑–∏ + –æ—Ç—á–µ—Ç—ã)</li>
                <li>–ö–æ–Ω—Ç–µ–∫—Å—Ç—ã –º–æ–≥—É—Ç –±—ã—Ç—å <strong>–±–æ–ª–µ–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –±–æ–≥–∞—Ç—ã–º–∏</strong>, —á–µ–º –ø—Ä–æ—Å—Ç—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —á–∞–Ω–∫–∏</li>
                <li>–ú–µ—Ç—Ä–∏–∫–∏ <strong>context_precision</strong> –∏ <strong>context_recall</strong> –æ—Ü–µ–Ω–∏–≤–∞—é—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≥—Ä–∞—Ñ–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã</li>
                <li><strong>answer_relevancy</strong> –∏—Å–ø–æ–ª—å–∑—É–µ—Ç embeddings –∏ –Ω–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞</li>
            </ul>
        </div>
    </div>
    """
    
    display(HTML(html))
    
    return {
        "compatibility": compatibility,
        "recommended_metrics": recommended_metrics,
        "has_contexts": has_contexts,
        "has_ground_truth": has_ground_truth,
        "stats": {
            "total_records": len(df),
            "avg_contexts": avg_contexts,
            "avg_entities": df['num_entities'].mean(),
            "avg_relationships": df['num_relationships'].mean()
        }
    }
```

### –û—Ü–µ–Ω–∫–∞ GraphRAG —Å RAGAS


```python
def evaluate_graphrag_with_ragas(
    dataset: Optional[Union[pd.DataFrame, str]] = None,
    graphrag_adapter: GraphRAGAdapter = None,
    test_cases: List[Dict[str, str]] = None,
    judge_model: str = "openai/gpt-5-mini",
    judge_api_key: str = None,
    judge_api_base: str = "https://api.vsegpt.ru/v1",
    embedding_model: str = "emb-openai/text-embedding-3-large",
    metrics: List[str] = None,
    max_tokens: int = 32000,
    save_dataset_path: Optional[str] = None,
    enable_timing: bool = True,
    analyze_compatibility: bool = True
) -> Dict[str, Any]:
    """
    –ü–æ–ª–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ GraphRAG —Å–∏—Å—Ç–µ–º—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAGAS 0.4+.
    
    Args:
        dataset: DataFrame –∏–ª–∏ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –≥–æ—Ç–æ–≤—ã–º –¥–∞—Ç–∞—Å–µ—Ç–æ–º
        graphrag_adapter: GraphRAGAdapter (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç)
        test_cases: –¢–µ—Å—Ç–æ–≤—ã–µ –∫–µ–π—Å—ã (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç)
        judge_model: –ú–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        judge_api_key: API –∫–ª—é—á –¥–ª—è Judge LLM
        judge_api_base: Base URL –¥–ª—è Judge LLM API
        embedding_model: –ú–æ–¥–µ–ª—å embeddings
        metrics: –°–ø–∏—Å–æ–∫ –º–µ—Ç—Ä–∏–∫ (None = –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
        max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è LLM
        save_dataset_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        enable_timing: –í–∫–ª—é—á–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        analyze_compatibility: –ü—Ä–æ–≤–µ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏
    """
    if not RAGAS_AVAILABLE:
        raise ImportError("RAGAS –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install ragas --upgrade")
    
    # 1. –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    print("=" * 80)
    print("üìä –®–ê–ì 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ GraphRAG")
    print("=" * 80)
    
    if dataset is not None:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç
        if isinstance(dataset, str):
            df = load_evaluation_dataset(dataset)
        else:
            df = dataset
            print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π DataFrame ({len(df)} –∑–∞–ø–∏—Å–µ–π)")
    else:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        if graphrag_adapter is None or test_cases is None:
            raise ValueError("–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å –ª–∏–±–æ dataset, –ª–∏–±–æ graphrag_adapter+test_cases")
        
        df = prepare_graphrag_evaluation_dataset(
            graphrag_adapter=graphrag_adapter,
            test_cases=test_cases,
            save_path=save_dataset_path,
            verbose=True
        )
    
    # –ê–Ω–∞–ª–∏–∑ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    if analyze_compatibility:
        print("\n" + "=" * 80)
        print("üîç –®–ê–ì 1.5: –ê–Ω–∞–ª–∏–∑ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ RAGAS")
        print("=" * 80)
        
        compatibility_result = analyze_graphrag_ragas_compatibility(df)
        
        # –ï—Å–ª–∏ –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ
        if metrics is None:
            metrics = compatibility_result["recommended_metrics"]
            print(f"\n‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±—Ä–∞–Ω—ã –º–µ—Ç—Ä–∏–∫–∏: {', '.join(metrics)}")
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç RAGAS
    queries, answers, contexts_list, ground_truths = dataset_to_ragas_format(df)
    
    # 2. –°–æ–∑–¥–∞–µ–º –æ—Ü–µ–Ω—â–∏–∫–∞ (Judge LLM)
    print("\n" + "=" * 80)
    print("ü§ñ –®–ê–ì 2: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Judge LLM –¥–ª—è –æ—Ü–µ–Ω–∫–∏")
    print("=" * 80)
    
    if judge_api_key is None:
        judge_api_key = os.getenv("OPENAI_API_KEY")
        if not judge_api_key:
            raise ValueError("–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å judge_api_key –∏–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å OPENAI_API_KEY")
    
    evaluation_client = UniversalLLMClient(
        model=judge_model,
        api_key=judge_api_key,
        api_base=judge_api_base,
        temperature=0.0,
        max_tokens=max_tokens
    )
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã
    if metrics is None:
        has_ground_truth = any(gt and gt.strip() for gt in ground_truths)
        
        if has_ground_truth:
            metrics = [
                "faithfulness",
                "answer_relevancy",
                "context_precision",
                "context_recall",
                "context_relevance",
                "response_groundedness",
                "answer_accuracy"
            ]
        else:
            metrics = [
                "faithfulness",
                "context_relevance",
                "response_groundedness",
                "answer_relevancy"
            ]
    
    evaluator = UniversalRAGEvaluator(
        judge_llm_client=evaluation_client,
        embedding_model=embedding_model,
        metrics=metrics,
        enable_timing=enable_timing
    )
    
    # 3. –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Ü–µ–Ω–∫—É
    print("\n" + "=" * 80)
    print("üöÄ –®–ê–ì 3: –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ RAGAS –¥–ª—è GraphRAG")
    print("=" * 80)
    print(f"‚öôÔ∏è  –ú–µ—Ç—Ä–∏–∫–∏: {', '.join(metrics)}")
    print(f"‚öôÔ∏è  max_tokens: {max_tokens}")
    print(f"‚è∞ –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç...\n")
    
    results = evaluator.evaluate(
        queries=queries,
        answers=answers,
        contexts=contexts_list,
        ground_truths=ground_truths,
        show_progress=True
    )
    
    # 4. –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    if enable_timing:
        print("\n" + "=" * 80)
        print("‚è±Ô∏è –®–ê–ì 3.5: –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ—Ç—Ä–∏–∫")
        print("=" * 80)
        
        evaluator.display_timing_analysis()
    
    # 5. –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\n" + "=" * 80)
    print("üìä –®–ê–ì 4: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ GraphRAG")
    print("=" * 80)
    
    evaluator.display_results(max_examples=min(10, len(queries)))
    
    # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\n" + "=" * 80)
    print("üíæ –®–ê–ì 5: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    print("=" * 80)
    
    results_path = evaluator.save_results()
    print(f"‚úÖ –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_path}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    metrics_path = Path("results") / f"graphrag_ragas_metrics_{timestamp}.json"
    metrics_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(metrics_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"‚úÖ –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏: {metrics_path}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    if enable_timing:
        timing_df = evaluator.get_timing_statistics()
        timing_path = Path("results") / f"graphrag_ragas_timing_{timestamp}.csv"
        timing_df.to_csv(timing_path, index=False)
        print(f"‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {timing_path}")
    
    # –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ GraphRAG
    print("\n" + "=" * 80)
    print("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ GraphRAG")
    print("=" * 80)
    print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ –∑–∞–ø—Ä–æ—Å: {df['num_contexts'].mean():.1f}")
    print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ —Å—É—â–Ω–æ—Å—Ç–µ–π: {df['num_entities'].mean():.1f}")
    print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ —Å–≤—è–∑–µ–π: {df['num_relationships'].mean():.1f}")
    print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –æ—Ç—á–µ—Ç–æ–≤: {df['num_reports'].mean():.1f}")
    print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –∑–∞–ø—Ä–æ—Å–∞: {df['elapsed_time'].mean():.1f}—Å")
    
    return {
        "aggregated_metrics": results,
        "detailed_results": evaluator.get_detailed_results(),
        "evaluator": evaluator,
        "dataset": df,
        "timing_statistics": evaluator.get_timing_statistics() if enable_timing else None,
        "graphrag_stats": {
            "avg_contexts": df['num_contexts'].mean(),
            "avg_entities": df['num_entities'].mean(),
            "avg_relationships": df['num_relationships'].mean(),
            "avg_reports": df['num_reports'].mean(),
            "avg_query_time": df['elapsed_time'].mean()
        }
    }
      
```

### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ RAG vs GraphRAG


```python
def compare_rag_vs_graphrag(
    rag_results: Dict[str, Any],
    graphrag_results: Dict[str, Any],
    save_path: Optional[str] = None
) -> pd.DataFrame:
    """
    –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –æ–±—ã—á–Ω–æ–≥–æ RAG –∏ GraphRAG.
    
    Args:
        rag_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ RAG
        graphrag_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ GraphRAG
        save_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    
    Returns:
        DataFrame —Å–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ–º
    """
    from IPython.display import display, HTML
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
    rag_metrics = rag_results["aggregated_metrics"]
    graphrag_metrics = graphrag_results["aggregated_metrics"]
    
    # –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    common_metrics = set(
        [k.replace('_mean', '') for k in rag_metrics.keys() if k.endswith('_mean')]
    ) & set(
        [k.replace('_mean', '') for k in graphrag_metrics.keys() if k.endswith('_mean')]
    )
    
    # –°–æ–∑–¥–∞–µ–º —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
    comparison_data = []
    
    for metric in sorted(common_metrics):
        rag_mean = rag_metrics.get(f"{metric}_mean", 0)
        graphrag_mean = graphrag_metrics.get(f"{metric}_mean", 0)
        
        difference = graphrag_mean - rag_mean
        improvement = (difference / rag_mean * 100) if rag_mean > 0 else 0
        
        comparison_data.append({
            "Metric": metric,
            "RAG": f"{rag_mean:.3f}",
            "GraphRAG": f"{graphrag_mean:.3f}",
            "Difference": f"{difference:+.3f}",
            "Improvement %": f"{improvement:+.1f}%",
            "Winner": "GraphRAG" if graphrag_mean > rag_mean else ("RAG" if rag_mean > graphrag_mean else "Tie")
        })
    
    df_comparison = pd.DataFrame(comparison_data)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    if save_path:
        save_path = Path(save_path)
        save_path.parent.mkdir(parents=True, exist_ok=True)
        df_comparison.to_csv(save_path, index=False)
        print(f"üíæ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {save_path}")
    
    # HTML –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    html = f"""
    <div style="border: 2px solid #9C27B0; border-radius: 12px; padding: 20px; margin: 15px 0;
                background: linear-gradient(135deg, #F3E5F5 0%, #E1BEE7 100%);">
        <h2 style="color: #6A1B9A; margin-top: 0;">‚öñÔ∏è –°—Ä–∞–≤–Ω–µ–Ω–∏–µ RAG vs GraphRAG</h2>
        
        <table style="width: 100%; border-collapse: collapse; background: white; margin: 15px 0;">
            <thead>
                <tr style="background: #9C27B0; color: white;">
                    <th style="padding: 12px; text-align: left;">–ú–µ—Ç—Ä–∏–∫–∞</th>
                    <th style="padding: 12px; text-align: center;">RAG</th>
                    <th style="padding: 12px; text-align: center;">GraphRAG</th>
                    <th style="padding: 12px; text-align: center;">–†–∞–∑–Ω–∏—Ü–∞</th>
                    <th style="padding: 12px; text-align: center;">–£–ª—É—á—à–µ–Ω–∏–µ</th>
                    <th style="padding: 12px; text-align: center;">–ü–æ–±–µ–¥–∏—Ç–µ–ª—å</th>
                </tr>
            </thead>
            <tbody>
    """
    
    for _, row in df_comparison.iterrows():
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ü–≤–µ—Ç —Å—Ç—Ä–æ–∫–∏
        if row['Winner'] == 'GraphRAG':
            bg_color = "#C8E6C9"
            winner_badge = "üèÜ GraphRAG"
            winner_color = "#2E7D32"
        elif row['Winner'] == 'RAG':
            bg_color = "#FFE0B2"
            winner_badge = "üèÜ RAG"
            winner_color = "#E65100"
        else:
            bg_color = "#E0E0E0"
            winner_badge = "ü§ù Tie"
            winner_color = "#666"
        
        html += f"""
            <tr style="background: {bg_color};">
                <td style="padding: 10px; font-weight: bold;">{row['Metric']}</td>
                <td style="padding: 10px; text-align: center;">{row['RAG']}</td>
                <td style="padding: 10px; text-align: center;">{row['GraphRAG']}</td>
                <td style="padding: 10px; text-align: center; font-weight: bold;">{row['Difference']}</td>
                <td style="padding: 10px; text-align: center; font-weight: bold;">{row['Improvement %']}</td>
                <td style="padding: 10px; text-align: center;">
                    <span style="background: {winner_color}; color: white; padding: 4px 8px; border-radius: 12px; font-size: 0.9em;">
                        {winner_badge}
                    </span>
                </td>
            </tr>
        """
    
    html += """
            </tbody>
        </table>
        
        <div style="margin-top: 20px; padding: 15px; background: white; border-radius: 8px;">
            <h3 style="color: #6A1B9A;">üìà –û–±—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç</h3>
    """
    
    # –ü–æ–¥—Å—á–µ—Ç –ø–æ–±–µ–¥
    graphrag_wins = len(df_comparison[df_comparison['Winner'] == 'GraphRAG'])
    rag_wins = len(df_comparison[df_comparison['Winner'] == 'RAG'])
    ties = len(df_comparison[df_comparison['Winner'] == 'Tie'])
    
    html += f"""
            <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 15px; margin-top: 15px;">
                <div style="text-align: center; padding: 15px; background: #C8E6C9; border-radius: 8px;">
                    <div style="font-size: 2em; font-weight: bold; color: #2E7D32;">{graphrag_wins}</div>
                    <div style="color: #666;">GraphRAG –ø–æ–±–µ–¥</div>
                </div>
                <div style="text-align: center; padding: 15px; background: #FFE0B2; border-radius: 8px;">
                    <div style="font-size: 2em; font-weight: bold; color: #E65100;">{rag_wins}</div>
                    <div style="color: #666;">RAG –ø–æ–±–µ–¥</div>
                </div>
                <div style="text-align: center; padding: 15px; background: #E0E0E0; border-radius: 8px;">
                    <div style="font-size: 2em; font-weight: bold; color: #666;">{ties}</div>
                    <div style="color: #666;">–ù–∏—á—å–∏—Ö</div>
                </div>
            </div>
        </div>
    </div>
    """
    
    display(HTML(html))
    
    print("\nüìä –î–µ—Ç–∞–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞:")
    display(df_comparison)
    
    return df_comparison
```

## –†–∞—Å—á—ë—Ç –º–µ—Ç—Ä–∏–∫


```python
import logging
logging.basicConfig(level=logging.INFO)
```


```python
print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GraphRAG...")
```

    üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GraphRAG...
    


```python
import tiktoken

from graphrag.config.models.vector_store_schema_config import VectorStoreSchemaConfig
from graphrag.query.context_builder.entity_extraction import EntityVectorStoreKey
from graphrag.query.indexer_adapters import (
    read_indexer_covariates,
    read_indexer_entities,
    read_indexer_relationships,
    read_indexer_reports,
    read_indexer_text_units,
)
from graphrag.query.question_gen.local_gen import LocalQuestionGen
from graphrag.query.structured_search.local_search.mixed_context import (
    LocalSearchMixedContext,
)
from graphrag.query.structured_search.local_search.search import LocalSearch
from graphrag.vector_stores.lancedb import LanceDBVectorStore
```


```python
from graphrag.config.enums import ModelType
from graphrag.config.models.language_model_config import LanguageModelConfig
from graphrag.language_model.manager import ModelManager
from graphrag.tokenizer.get_tokenizer import get_tokenizer
```


```python
directory = 'C:\\Users\\glvv2\\vkr\\graph_local3' # os.getcwd()

INPUT_DIR = os.path.join(directory, "output")
LANCEDB_URI = os.path.join(INPUT_DIR, "lancedb")

COMMUNITY_REPORT_TABLE = "community_reports"
ENTITY_TABLE = "entities"
COMMUNITY_TABLE = "communities"
RELATIONSHIP_TABLE = "relationships"
COVARIATE_TABLE = "covariates"
TEXT_UNIT_TABLE = "text_units"
COMMUNITY_LEVEL = 2
```


```python
api_key="sk-or-vv-xxx"

llm_model = "google/gemma-3-27b-it"
embedding_model = "emb-openai/text-embedding-3-large"

chat_config = LanguageModelConfig(
    api_key=api_key,
    api_base="https://api.vsegpt.ru/v1",
    type=ModelType.Chat,
    model_provider="openai",
    model=llm_model,
    max_retries=20,
)

chat_model = ModelManager().get_or_create_chat_model(
    name="local_search",
    model_type=ModelType.Chat,
    config=chat_config,
)

embedding_config = LanguageModelConfig(
    api_key=api_key,
    api_base="https://api.vsegpt.ru/v1",
    type=ModelType.Embedding,
    model_provider="openai",
    model=embedding_model,
    max_retries=20,
)

text_embedder = ModelManager().get_or_create_embedding_model(
    name="local_search_embedding",
    model_type=ModelType.Embedding,
    config=embedding_config,
)

tokenizer = get_tokenizer(chat_config)

description_embedding_store = LanceDBVectorStore(
    vector_store_schema_config=VectorStoreSchemaConfig(
        index_name="default-entity-description"
    )
)
description_embedding_store.connect(db_uri=LANCEDB_URI)

entities = pd.read_parquet(f"{directory}/output/entities.parquet")
communities = pd.read_parquet(f"{directory}/output/communities.parquet")
community_reports = pd.read_parquet(f"{directory}/output/community_reports.parquet")
text_units = pd.read_parquet(f"{directory}/output/text_units.parquet")
relationships = pd.read_parquet(f"{directory}/output/relationships.parquet")

entities_d = read_indexer_entities(entities, communities, None) #, COMMUNITY_LEVEL)
relationships_d = read_indexer_relationships(relationships)
reports_d = read_indexer_reports(community_reports, communities, None) # , COMMUNITY_LEVEL)
text_units_d = read_indexer_text_units(text_units)

context_builder = LocalSearchMixedContext(
    community_reports=reports_d,
    text_units=text_units_d,
    entities=entities_d,
    relationships=relationships_d,
    covariates=None, #covariates,
    entity_text_embeddings=description_embedding_store,
    embedding_vectorstore_key=EntityVectorStoreKey.ID,
    text_embedder=text_embedder,
    tokenizer=tokenizer,
)

local_context_params = {
    "text_unit_prop": 0.5,
    "community_prop": 0.1,
    "conversation_history_max_turns": 5,
    "conversation_history_user_turns_only": True,
    "top_k_mapped_entities": 10,
    "top_k_relationships": 10,
    "include_entity_rank": True,
    "include_relationship_weight": True,
    "include_community_rank": False,
    "return_candidate_context": False, #True,
    "embedding_vectorstore_key": EntityVectorStoreKey.ID,
    "max_tokens": 24_000,
}

model_params = {
    "max_tokens": 3_000,
    "temperature": 0.0,
}

search_engine = LocalSearch(
    model=chat_model,
    context_builder=context_builder,
    tokenizer=tokenizer,
    model_params=model_params,
    context_builder_params=local_context_params,
    response_type="multiple paragraphs",
)
```


```python
start = time.time()

result = await search_engine.search("Draw conclusions about the i101 object in Attempto Controlled English")

end = time.time()
print(f"–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {(end - start):.3f} —Å–µ–∫.")
```

    [92m08:23:06 - LiteLLM:INFO[0m: utils.py:1575 - Wrapper: Completed Call, calling success_handler
    2026-01-10 08:23:06,010 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
    [92m08:23:06 - LiteLLM:INFO[0m: utils.py:3749 - 
    LiteLLM completion() model= google/gemma-3-27b-it; provider = openai
    2026-01-10 08:23:06,734 - LiteLLM - INFO - 
    LiteLLM completion() model= google/gemma-3-27b-it; provider = openai
    

    –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: 14.509 —Å–µ–∫.
    


```python
print("\nüì¶ –°–æ–∑–¥–∞–Ω–∏–µ –∞–¥–∞–ø—Ç–µ—Ä–∞ GraphRAG –¥–ª—è RAGAS...")

graphrag_adapter = GraphRAGAdapter(search_engine=search_engine)
```

    
    üì¶ –°–æ–∑–¥–∞–Ω–∏–µ –∞–¥–∞–ø—Ç–µ—Ä–∞ GraphRAG –¥–ª—è RAGAS...
    


```python
print("\nüìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∫–µ–π—Å–æ–≤...")

loaded_test_cases = load_test_cases_from_file("./datasets/rag_test_cases_20260109_222314.json")

print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(loaded_test_cases)} —Ç–µ—Å—Ç–æ–≤—ã—Ö –∫–µ–π—Å–æ–≤")
```

    
    üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∫–µ–π—Å–æ–≤...
    üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∫–µ–π—Å–æ–≤: datasets\rag_test_cases_20260109_222314.json
    ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ 198 —Ç–µ—Å—Ç–æ–≤—ã—Ö –∫–µ–π—Å–æ–≤ –∏–∑ JSON
    ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ 198 —Ç–µ—Å—Ç–æ–≤—ã—Ö –∫–µ–π—Å–æ–≤
    


```python
print("\nüìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ GraphRAG")

graphrag_dataset = prepare_graphrag_evaluation_dataset(
    graphrag_adapter=graphrag_adapter,
    test_cases=loaded_test_cases, #test_cases,
    save_path="./datasets/graphrag_evaluation_dataset.parquet",
    verbose=True
)
```


```python
print("üîç –ê–Ω–∞–ª–∏–∑ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏")

compatibility = analyze_graphrag_ragas_compatibility(graphrag_dataset)
```


```python
graphrag_dataset.contexts[0][:150]
```




    'Entity: Unknown Entity|||Entity: Unknown Entity | Description: I90 is a context-dependent shape representation and has a representation relation (i91)'




```python
metrics_eq = [
    # "faithfulness",
    "answer_relevancy",
    # "context_precision",
    "context_recall",
    "context_relevance",
    # "response_groundedness",
    "answer_accuracy"
]
```


```python
print("üöÄ –†–∞—Å—á—ë—Ç –º–µ—Ç—Ä–∏–∫ RAGAS –¥–ª—è GraphRAG")

graphrag_evaluation_results_eq = evaluate_graphrag_with_ragas(
    dataset=graphrag_dataset,
    judge_model="openai/gpt-5-mini",
    judge_api_key="sk-or-vv-xxx",
    judge_api_base="https://api.vsegpt.ru/v1",
    embedding_model="emb-openai/text-embedding-3-large",
    max_tokens=32000,
    enable_timing=True,
    analyze_compatibility=True,
    metrics=metrics_eq
)
```


```python
print("‚öñÔ∏è –°—Ä–∞–≤–Ω–µ–Ω–∏–µ RAG vs. GraphRAG")

try:
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ RAG
    # with open("results/ragas_metrics_20260109_183330.json", 'r') as f:
    #     rag_aggregated = json.load(f)
    # rag_results = {"aggregated_metrics": rag_aggregated}
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
    comparison_df_eq = compare_rag_vs_graphrag(
        rag_results=evaluation_results, # rag_results,
        graphrag_results=graphrag_evaluation_results_eq,
        save_path="./results/rag_vs_graphrag_comparison_eq.csv"
    )
    
except Exception as e:
    print(f"‚ö†Ô∏è  –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ: {e}")
```

    ‚öñÔ∏è –°—Ä–∞–≤–Ω–µ–Ω–∏–µ RAG vs. GraphRAG
    üíæ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: results\rag_vs_graphrag_comparison_eq.csv
    



<div style="border: 2px solid #9C27B0; border-radius: 12px; padding: 20px; margin: 15px 0;
            background: linear-gradient(135deg, #F3E5F5 0%, #E1BEE7 100%);">
    <h2 style="color: #6A1B9A; margin-top: 0;">‚öñÔ∏è –°—Ä–∞–≤–Ω–µ–Ω–∏–µ RAG vs GraphRAG</h2>

    <table style="width: 100%; border-collapse: collapse; background: white; margin: 15px 0;">
        <thead>
            <tr style="background: #9C27B0; color: white;">
                <th style="padding: 12px; text-align: left;">–ú–µ—Ç—Ä–∏–∫–∞</th>
                <th style="padding: 12px; text-align: center;">RAG</th>
                <th style="padding: 12px; text-align: center;">GraphRAG</th>
                <th style="padding: 12px; text-align: center;">–†–∞–∑–Ω–∏—Ü–∞</th>
                <th style="padding: 12px; text-align: center;">–£–ª—É—á—à–µ–Ω–∏–µ</th>
                <th style="padding: 12px; text-align: center;">–ü–æ–±–µ–¥–∏—Ç–µ–ª—å</th>
            </tr>
        </thead>
        <tbody>

        <tr style="background: #C8E6C9;">
            <td style="padding: 10px; font-weight: bold;">answer_accuracy</td>
            <td style="padding: 10px; text-align: center;">0.367</td>
            <td style="padding: 10px; text-align: center;">0.486</td>
            <td style="padding: 10px; text-align: center; font-weight: bold;">+0.119</td>
            <td style="padding: 10px; text-align: center; font-weight: bold;">+32.3%</td>
            <td style="padding: 10px; text-align: center;">
                <span style="background: #2E7D32; color: white; padding: 4px 8px; border-radius: 12px; font-size: 0.9em;">
                    üèÜ GraphRAG
                </span>
            </td>
        </tr>

        <tr style="background: #C8E6C9;">
            <td style="padding: 10px; font-weight: bold;">answer_relevancy</td>
            <td style="padding: 10px; text-align: center;">0.359</td>
            <td style="padding: 10px; text-align: center;">0.483</td>
            <td style="padding: 10px; text-align: center; font-weight: bold;">+0.124</td>
            <td style="padding: 10px; text-align: center; font-weight: bold;">+34.7%</td>
            <td style="padding: 10px; text-align: center;">
                <span style="background: #2E7D32; color: white; padding: 4px 8px; border-radius: 12px; font-size: 0.9em;">
                    üèÜ GraphRAG
                </span>
            </td>
        </tr>

        <tr style="background: #C8E6C9;">
            <td style="padding: 10px; font-weight: bold;">context_recall</td>
            <td style="padding: 10px; text-align: center;">0.116</td>
            <td style="padding: 10px; text-align: center;">0.727</td>
            <td style="padding: 10px; text-align: center; font-weight: bold;">+0.611</td>
            <td style="padding: 10px; text-align: center; font-weight: bold;">+525.8%</td>
            <td style="padding: 10px; text-align: center;">
                <span style="background: #2E7D32; color: white; padding: 4px 8px; border-radius: 12px; font-size: 0.9em;">
                    üèÜ GraphRAG
                </span>
            </td>
        </tr>

        <tr style="background: #C8E6C9;">
            <td style="padding: 10px; font-weight: bold;">context_relevance</td>
            <td style="padding: 10px; text-align: center;">0.551</td>
            <td style="padding: 10px; text-align: center;">0.975</td>
            <td style="padding: 10px; text-align: center; font-weight: bold;">+0.424</td>
            <td style="padding: 10px; text-align: center; font-weight: bold;">+77.1%</td>
            <td style="padding: 10px; text-align: center;">
                <span style="background: #2E7D32; color: white; padding: 4px 8px; border-radius: 12px; font-size: 0.9em;">
                    üèÜ GraphRAG
                </span>
            </td>
        </tr>

        <tr style="background: #C8E6C9;">
            <td style="padding: 10px; font-weight: bold;">overall</td>
            <td style="padding: 10px; text-align: center;">0.348</td>
            <td style="padding: 10px; text-align: center;">0.668</td>
            <td style="padding: 10px; text-align: center; font-weight: bold;">+0.320</td>
            <td style="padding: 10px; text-align: center; font-weight: bold;">+91.8%</td>
            <td style="padding: 10px; text-align: center;">
                <span style="background: #2E7D32; color: white; padding: 4px 8px; border-radius: 12px; font-size: 0.9em;">
                    üèÜ GraphRAG
                </span>
            </td>
        </tr>

        </tbody>
    </table>

    <div style="margin-top: 20px; padding: 15px; background: white; border-radius: 8px;">
        <h3 style="color: #6A1B9A;">üìà –û–±—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç</h3>

        <div style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 15px; margin-top: 15px;">
            <div style="text-align: center; padding: 15px; background: #C8E6C9; border-radius: 8px;">
                <div style="font-size: 2em; font-weight: bold; color: #2E7D32;">5</div>
                <div style="color: #666;">GraphRAG –ø–æ–±–µ–¥</div>
            </div>
            <div style="text-align: center; padding: 15px; background: #FFE0B2; border-radius: 8px;">
                <div style="font-size: 2em; font-weight: bold; color: #E65100;">0</div>
                <div style="color: #666;">RAG –ø–æ–±–µ–¥</div>
            </div>
            <div style="text-align: center; padding: 15px; background: #E0E0E0; border-radius: 8px;">
                <div style="font-size: 2em; font-weight: bold; color: #666;">0</div>
                <div style="color: #666;">–ù–∏—á—å–∏—Ö</div>
            </div>
        </div>
    </div>
</div>



    
    üìä –î–µ—Ç–∞–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞:
    


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Metric</th>
      <th>RAG</th>
      <th>GraphRAG</th>
      <th>Difference</th>
      <th>Improvement %</th>
      <th>Winner</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>answer_accuracy</td>
      <td>0.367</td>
      <td>0.486</td>
      <td>+0.119</td>
      <td>+32.3%</td>
      <td>GraphRAG</td>
    </tr>
    <tr>
      <th>1</th>
      <td>answer_relevancy</td>
      <td>0.359</td>
      <td>0.483</td>
      <td>+0.124</td>
      <td>+34.7%</td>
      <td>GraphRAG</td>
    </tr>
    <tr>
      <th>2</th>
      <td>context_recall</td>
      <td>0.116</td>
      <td>0.727</td>
      <td>+0.611</td>
      <td>+525.8%</td>
      <td>GraphRAG</td>
    </tr>
    <tr>
      <th>3</th>
      <td>context_relevance</td>
      <td>0.551</td>
      <td>0.975</td>
      <td>+0.424</td>
      <td>+77.1%</td>
      <td>GraphRAG</td>
    </tr>
    <tr>
      <th>4</th>
      <td>overall</td>
      <td>0.348</td>
      <td>0.668</td>
      <td>+0.320</td>
      <td>+91.8%</td>
      <td>GraphRAG</td>
    </tr>
  </tbody>
</table>
</div>



```python
print("\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã GraphRAG:")
for metric, value in graphrag_evaluation_results_eq["aggregated_metrics"].items():
    if metric.endswith('_mean'):
        metric_name = metric.replace('_mean', '')
        print(f"   ‚Ä¢ {metric_name}: {value:.3f}")
```

    
    üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã GraphRAG:
       ‚Ä¢ answer_relevancy: 0.483
       ‚Ä¢ context_recall: 0.727
       ‚Ä¢ context_relevance: 0.975
       ‚Ä¢ answer_accuracy: 0.486
       ‚Ä¢ overall: 0.668
    


```python
print("\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ GraphRAG:")
stats = graphrag_evaluation_results_eq["graphrag_stats"]
print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤: {stats['avg_contexts']:.1f}")
print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ —Å—É—â–Ω–æ—Å—Ç–µ–π: {stats['avg_entities']:.1f}")
print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ —Å–≤—è–∑–µ–π: {stats['avg_relationships']:.1f}")
print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –∑–∞–ø—Ä–æ—Å–∞: {stats['avg_query_time']:.1f}—Å")
```

    
    üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ GraphRAG:
       ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤: 80.7
       ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ —Å—É—â–Ω–æ—Å—Ç–µ–π: 20.0
       ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ —Å–≤—è–∑–µ–π: 45.9
       ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –∑–∞–ø—Ä–æ—Å–∞: 14.4—Å
    


```python
print("\nüíæ –§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ 'results/'")
```

    
    üíæ –§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ 'results/'
    


```python

```
